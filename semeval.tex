\PassOptionsToPackage{table}{xcolor}
\documentclass[11pt]{article}
\usepackage[final]{acl}
\usepackage{times}
\usepackage{multirow}
\usepackage{array}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\makeatletter
\providecommand{\do@row@strut}{}
\makeatother


\setlist[itemize]{noitemsep,topsep=2pt,leftmargin=*}
\setlist[enumerate]{noitemsep,topsep=2pt,leftmargin=*}

% Page Count Display Configuration
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[C]{\small Page \thepage\ of \pageref{LastPage}}


\lstset{
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  breakatwhitespace=false,
  frame=single,
  framesep=2pt,
  columns=fullflexible,
  keepspaces=true,
  xleftmargin=2pt,
  xrightmargin=2pt,
  aboveskip=4pt,
  belowskip=4pt,
  escapeinside={(*@}{@*)},
}


\usetikzlibrary{positioning, arrows.meta, fit, backgrounds, shapes.geometric, calc}


\title{AILS-NTUA at SemEval-2026 Task 10: Agentic LLMs for Psycholinguistic Marker Extraction and Conspiracy Endorsement Detection}

% Qwen 3 8b
% 02-13 19:36:13.668 | INFO     | __main__:main_async:282 - FINAL RESULTS
% 2026-02-13 19:36:13.668 | INFO     | __main__:main_async:283 -   S1 Macro Overlap F1: {'Actor_f1': 0.19178082191780824, 'Effect_f1': 0.12962962962962962, 'Action_f1': 0.19999999999999998, 'Evidence_f1': 0.07692307692307693, 'Victim_f1': 0.19148936170212766, 'macro_f1': 0.15796457803452849}
% 2026-02-13 19:36:13.669 | INFO     | __main__:main_async:288 -   S2 Weighted F1: 0.6322
% 2026-02-13 19:36:13.673 | INFO     | __main__:main_async:291 -
%               precision    recall  f1-score   support
% 
%   conspiracy       0.83      0.19      0.30        27
%          non       0.69      0.98      0.81        50
% 
%     accuracy                           0.70        77
%    macro avg       0.76      0.58      0.56        77
% weighted avg       0.74      0.70      0.63        77

\author{Panagiotis Spanakis \hspace{0.8em} Maria Lymperaiou \hspace{0.8em} Giorgos Filandrianos \\
Athanasios Voulodimos \hspace{0.8em} Giorgos Stamou \\
National Technical University of Athens (NTUA)}

\begin{document}
\maketitle
\thispagestyle{fancy}

\begin{abstract}
	This paper presents a novel agentic LLM architecture for SemEval-2026 Task 10 that jointly extracts psycholinguistic conspiracy markers (S1) and detects conspiracy endorsement (S2).
	Unlike traditional classifiers that conflate semantic reasoning with structural localization, our two-stage decoupled design isolates these challenges.
	For S1, we propose Dynamic Discriminative Chain-of-Thought (DD-CoT) with deterministic anchoring to resolve semantic ambiguity and character-level brittleness.
	For S2, an ``Anti-Echo Chamber'' architecture, consisting of an adversarial Parallel Council adjudicated by a Calibrated Judge, overcomes the ``Reporter Trap,'' where models falsely penalize objective reporting.
	Achieving 0.24 Macro F1 (+100\% over baseline) on S1 and 0.79 Macro F1 (+49\%) on S2, our approach establishes a robust paradigm for interpretable, psycholinguistically-grounded NLP.
\end{abstract}

\section{Introduction}
Humans have long exhibited a tendency to endorse conspiracy theories,
particularly in contexts of uncertainty, threat, and social upheaval. Such
beliefs are created and distributed to support human need for addressing
existential or social issues and strengthen their sense of identity and
belonging \cite{psychology-of-conspiracy, belief}. Despite their psychological
appeal, conspiracies are associated with harmful consequences, limiting trust
in well-documented facts and public decisions, while exacerbating political
polarization and misinformation patterns \cite{understanding}.

The rise of artificial intelligence inspired the connection of conspiracy
identification with natural language, as language constitutes the most
fundamental medium for conspiracy articulation and spread. Conspiratorial
statements are subtly embedded in language exploiting linguistic strategies to
evoke emotions and attribution of agency \cite{miani2022loco,
	rains2023psycholinguistic}, suggesting that conspiracy identification goes well
beyond superficial linguistic cues.

Large Language Models (LLMs) have revolutionized linguistic research, enabling
deep pattern identification and discrimination among their numerous abilities.
However, LLMs have been found to be significantly prone to cognitive biases
\cite{filandrianos-etal-2025-bias} and manipulation via persuasive language
\cite{xu-etal-2024-earth}, while they generate and amplify misinformation
\cite{chen2024combatingmisinformation}. Going one step further,
state-of-the-art LLMs are even able to persuade people to adopt conspiratorial
beliefs to a comparable degree as they can mitigate conspiracy dissemination
\cite{costello2026largelanguagemodelseffectively}, exposing the double-edged
nature of LLMs in the context of factual verification.

The core challenges that conspiratorial discourse poses call for fine-grained
data approaches that allow delving into the linguistic mechanisms that
characterize conspiratorial utterances in an interpretable way. Nevertheless,
prior datasets \cite{shahsavari2020conspiracy, langguth2023coco} frame
conspiracy detection as a coarse-grained classification task, abstracting away
from the particularities of conspiratorial discourse, thus obscuring how
conspiratorial reasoning is formed and expressed in language. To fill this gap,
the \textbf{SemEval 2026 Task 10: Psycholinguistic Conspiracy Marker Extraction
	and Detection} emphasizes the localization and categorization of linguistic
markers that signal conspiratorial thinking, complementing detection with
psychologically-backed annotations.

To address the dual challenges of accurate detection and interpretable marker
extraction, models must be capable of capturing both global conspiratorial
intent and fine-grained psycholinguistic cues embedded in language. In our
approach, we leverage LLMs within agentic structures to advance the recognition
of conspiratorial thought. Specifically, we employ our proposed \textbf{Dynamic
	Discriminative Chain-of-Thought (DD-CoT)} framework which adapts the adaptive
nature of Dynamic CoT \cite{ma2024dynamic} to perform semantic discrimination
with deterministic verification for precise marker extraction and an
``Anti-Echo Chamber'' council of contrasting perspectives to separate
conspiracy endorsement from neutral reporting. To the best of our knowledge, we
introduce the \textit{first agentic LLM-based method} to combat conspiracy
detection and identification of psycholinguistic features in language.

In short, our contributions are the following:
\begin{itemize}
	\item We introduce the first agentic LLM-based method for psycholinguistic conspiracy
	      marker extraction and detection.
	\item We propose \textbf{Dynamic Discriminative Chain-of-Thought (DD-CoT)}, a
	      reasoning strategy that extends adaptive frameworks \cite{ma2024dynamic} by
	      forcing explicit counter-arguments to reduce semantic confusion.
	\item We propose a hybrid extraction architecture that decouples LLM semantic
	      decisions (what to extract / stance interpretation) from deterministic span
	      localization and lightweight profiling, improving reliability in
	      character-accurate outputs.
	\item We provide a comprehensive empirical analysis of key workflow components, including
	      computational latency profiling, robustness to hyperparameter variations, and juror ablation studies,
	      and demonstrate partial transfer to an 8B open-weights setting via simplified
	      agents.
\end{itemize}

The code for our system is available on GitHub\footnote{\url{https://github.com/panos-span/PsyChoMark_Semeval}}.

\section{Background}
\paragraph{Task description}
The dataset comprises 4,800 annotations spanning 4,100 unique Reddit submission
statements from $>$190 subreddits, divided in two subtasks: \textbf{\textit{i)}
	S1: Conspiracy Marker Extraction} contains textual spans that express core
conspiracy markers grounded in evolutionary psychology. One or more marker
types may appear in each comment, falling in the following categories:
\textsc{Actor} (mentions of individual or group agents), \textsc{Action}
(descriptions of what the actor is doing), \textsc{Effect} (consequences of the
actions), \textsc{Victim} (who is being harmed), \textsc{Evidence} (claims or
proof used to support the theory). \textbf{\textit{ii)} S2: Conspiracy
	Detection} assigns conspiracy-related or not conspiracy-related labels to
Reddit comments. More details about the dataset are provided in App.
\ref{sec:eda}.

\paragraph{Related work}
Early works on NLP conspiratorial discourse on Reddit introduced narrative
motifs correlated with conspiratorial evidence \cite{online-discussions} and
demonstrated that conspiratorial thinking manifests through detectable
psycholinguistic signals in user language \cite{Klein2019Pathways}, with
consequent literature revealing that conspiracy theories exhibit distinctive
narrative frameworks that can be computationally extracted from text
\cite{Tangherlini2020Automated}. These foundational works empowered the
operationalization of conspiracy identification as a classification task,
exemplified by datasets such as COCO \cite{langguth2023coco} and YouNICon
\cite{YouNICon}. Conspiracy detection involves techniques that explicitly model
psycholinguistic signals, such as affective tone, attributional cues and
explanatory framing, in order to provide explanatory evidence of conspiracy
presence in language \cite{rains2023psycholinguistic,
	language-of-conspiracy-theories, marino-etal-2025-linguistic}. The strong
contextualization that LLMs offer inspired the introduction of related
approaches; leveraging appropriate prompting enables accurate multi-label
conspiracy classification, eliminating training demands
\cite{peskine-etal-2023-definitions}. Complementarily, ConspEmoLLM
\cite{liu2024conspemollm} involves emotion-aware LLM fine-tuning on several
conspiratorial tasks, improving detection by leveraging affective signals, with
subsequent extensions focusing on robustness to stylistic and emotional
variation \cite{liu2025conspemollmv2}. Recent evaluations indicate that
LLM-based conspiracy detection often relies on topical shortcuts and struggles
with narrative ambiguity, underscoring the need for approaches grounded in
interpretable psycholinguistic markers \cite{pustet-etal-2024-detection,
	classifying}.

\section{System Overview}

We implement a two-stage agentic workflow: \textbf{S1} extracts
psycholinguistic marker spans, and \textbf{S2} predicts conspiracy endorsement
conditioned on the document and the extracted markers. The design separates (i)
LLM-mediated semantic decisions (what to extract / how to interpret stance)
from (ii) deterministic operations that require exactness (character offsets,
lightweight text statistics). Figure~\ref{fig:arch} summarizes the inference
flow.

\begin{figure*}[t]
	\vspace{-6pt}
	\centering
	\scriptsize
	\begin{tikzpicture}[
		node distance=0.25cm and 0.15cm,
		>={Stealth[length=3pt]},
		% Styles
		inputbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.4cm,
				minimum width=1.1cm, align=center, font=\tiny\sffamily, fill=gray!10},
		ragbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.4cm,
				minimum width=1.1cm, align=center, font=\tiny\sffamily, fill=green!10},
		llmbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.4cm,
				minimum width=1.1cm, align=center, font=\tiny\sffamily, fill=blue!10},
		detbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.4cm,
				minimum width=1.1cm, align=center, font=\tiny\sffamily, fill=orange!15},
		outputbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.4cm,
				minimum width=1.1cm, align=center, font=\tiny\sffamily, thick},
		arrow/.style={->, thick, black!70, rounded corners=2pt}, line/.style={-, thick,
				black!70, rounded corners=2pt},
		grouplab/.style={font=\bfseries\scriptsize\sffamily, text=black!70}, ]% Non-arrow line

		% === INPUT ===
		\node[inputbox] (input) {Document};

		% === S1 PIPELINE (Top Row) ===
		\node[ragbox, right=0.6cm of input] (s1rag) {Stratified\\Few-shot\\Retrieval};
		\node[llmbox, right=0.35cm of s1rag] (gen) {DD-CoT\\Generator\\{\tiny $\tau$=0.7}};
		\node[llmbox, right=0.35cm of gen] (critic) {Enhanced\\Critic\\{\tiny $\tau$=0.0}};
		\node[llmbox, right=0.35cm of critic] (refiner) {Refiner\\Agent\\{\tiny $\tau$=0.0}};
		\node[detbox, right=0.35cm of refiner] (verifier) {Deterministic\\Verifier\\{\tiny $\tau$=0.0}};
		\node[outputbox, right=0.35cm of verifier] (s1out) {S1 Spans};

		% S1 Connections
		\draw[arrow] (input) -- (s1rag);
		\draw[arrow] (s1rag) -- (gen);
		\draw[arrow] (gen) -- (critic);
		\draw[arrow] (critic) -- (refiner);
		\draw[arrow] (refiner) -- (verifier);
		\draw[arrow] (verifier) -- (s1out);

		% S1 Group Box
		\begin{scope}[on background layer]
			\node[draw=blue!40, fill=blue!3, rounded corners=4pt, dashed,
			fit=(s1rag)(gen)(critic)(refiner)(verifier)(s1out),
			inner sep=6pt, label={[grouplab]above:{S1: Marker Extraction (Self-Refine + DD-CoT)}}] (s1group) {};
		\end{scope}

		% === S2 PIPELINE (Bottom Row) ===

		% Left side components
		\node[ragbox, below=1.4cm of s1rag] (s2rag) {Contrastive\\Few-shot\\Retrieval};
		\node[detbox, right=0.3cm of s2rag] (forensic) {Forensic\\Profiler};

		% Parallel Council (Vertical Stack)
		% Positioned further right to allow space for the "Bus" lines
		\node[llmbox, right=1.6cm of forensic, yshift=1.0cm] (pros) {Prosecutor\\{\tiny $\tau$=0.4}};
		\node[llmbox, below=0.15cm of pros] (defense) {Defense\\{\tiny $\tau$=0.4}};
		\node[llmbox, below=0.15cm of defense] (literal) {Literalist\\{\tiny $\tau$=0.4}};
		\node[llmbox, below=0.15cm of literal] (stance) {Stance\\Profiler\\{\tiny $\tau$=0.4}};

		% Parallel Council Group Box
		\begin{scope}[on background layer]
			\node[draw=blue!40, fill=blue!3, rounded corners=3pt, dashed,
			fit=(pros)(defense)(literal)(stance),
			inner sep=6pt, label={[grouplab, font=\small]above:{Parallel Council}}] (pcgroup) {};
		\end{scope}

		% Judge & Verdict
		\coordinate (council_center) at ($(pros.north)!0.5!(stance.south)$);
		\node[llmbox, right=0.8cm of council_center -| stance.east] (judge) {Calibrated\\Judge\\{\tiny $\tau$=0.0}};
		\node[outputbox, right=0.35cm of judge] (verdict) {Final\\Verdict};

		% === WIRING THE BUS (The "Spine" Solution) ===

		% Define the horizontal position of the vertical spine
		\coordinate (spine_x) at ($(forensic.east)!0.6!(defense.west)$);

		% Calculate top and bottom of the spine
		\coordinate (spine_top) at (spine_x |- s1out.south); % Start at S1 height
		\coordinate (spine_bot) at (spine_x |- stance.west); % End at lowest agent

		% 1. Draw the Vertical Spine (The visible backbone)
		% We extend it up slightly to catch the S1 input
		\draw[line] ($(spine_top) + (0, -0.5)$) -- (spine_bot);

		% 2. Connect Inputs TO the Spine
		% S1 Spans (Drops down, then hits spine)
		\draw[arrow] (s1out.south) -- ++(0,-0.5) -| (spine_x);

		% Forensic (Straight right to spine)
		\draw[arrow] (forensic.east) -- (forensic.east -| spine_x);

		% S2 Retrieval (Up and right to spine) - bypassing Forensic
		\draw[arrow] (s2rag.north) -- ++(0,0.3) -| (spine_x);

		% Input Document to S2
		\draw[arrow] (input.south) |- (s2rag.west);
		\draw[arrow] (input.south) |- ($(s2rag.south) + (0,-0.3)$) -| (forensic.south);

		% 3. Connect Spine FROM the Spine TO Agents
		\foreach \agent in {pros, defense, literal, stance} {
				% Draw from the intersection of the spine and the agent's height
				\draw[arrow] (spine_x |- \agent.west) -- (\agent.west);
			}

		% 4. Connect Agents TO Judge
		\foreach \agent in {pros, defense, literal, stance} {
				\draw[arrow] (\agent.east) -- ++(0.2,0) |- (judge.west);
			}

		% Judge to Verdict
		\draw[arrow] (judge) -- (verdict);

		% S2 Group Box
		\begin{scope}[on background layer]
			\node[draw=red!40, fill=red!3, rounded corners=4pt, dashed,
			fit=(s2rag)(forensic)(pros)(stance)(judge)(verdict),
			inner sep=6pt, label={[grouplab]below:{S2: Anti-Echo Chamber (Parallel Council)}}] (s2group) {};
		\end{scope}

	\end{tikzpicture}
	\vspace{-4pt}
	\caption{System architecture overview. \textbf{S1} (top): DD-CoT Self-Refine extracts marker strings, then a deterministic verifier anchors them to character offsets. \textbf{S2} (bottom): an Anti-Echo Chamber comprising contrastive retrieval, forensic profiling, a \emph{Parallel Council} (Prosecutor, Defense, Literalist, Stance Profiler), and a calibrated Judge predicts conspiracy endorsement.}
	\label{fig:arch}
	\vspace{-10pt}
\end{figure*}

\subsection{Contrastive Few-shot Retrieval}
\label{sec:contrastive-rag}
In-context few-shot examples are retrieved for the given document (no augmentation
is performed). A contrastive strategy is optionally used to supply
discriminative precedents (including hard negatives for the Reporter Trap).
For S1, retrieval is additionally \emph{stratified}: we balance positive/negative documents and upweight underrepresented marker types (e.g., \textsc{Evidence}, \textsc{Victim}) so that prompts include instructive cases beyond the dominant \textsc{Actor}/\textsc{Action} patterns.
For S2, hard negatives are \texttt{non} documents that nevertheless contain S1
marker vocabulary; they match topic but oppose stance, forcing deliberation to
attend to attribution, hedging, and framing cues rather than topical overlap.
This targets Reporter Trap errors by construction.
Full retrieval specification, weights, reranking, and Figure~\ref{fig:rag} are
preserved in Appendix~\ref{app:rag-details}.

\subsection{S1: Marker Extraction via DD-CoT}

S1 produces a set of labeled spans by combining a self-refinement loop with a
deterministic span locator. The graph consumes the document and retrieved
few-shot precedents, generates candidate marker \emph{strings} with labels,
iteratively corrects them, then anchors each string to \texttt{startIndex} and
\texttt{endIndex} in the original text.

We explicitly decouple \textit{semantic identification} from \textit{span
	indexing}. LLMs can justify category assignments but are brittle at
character-accurate localization \cite{fu2024struggle}. We therefore (i) ask the
LLM to emit verbatim marker strings with labels, and (ii) compute offsets with
a deterministic locator that performs exact matching against the source text.
This avoids ``hallucinated spans'' and off-by-one indices while preserving the
LLM's interpretive signal \cite{ogasa-arase-2025-hallucinated}.

\paragraph{Dynamic Discriminative Chain-of-Thought (DD-CoT).} We introduce DD-CoT, which extends the adaptive reasoning of Dynamic CoT
\cite{ma2024dynamic} with an explicit \emph{discrimination step}.
\emph{discrimination step}. For each candidate span, the generator must state
(i) evidence for the chosen label and (ii) a short counter-argument against at
least one confusable label. This forces the model to commit to a decision
boundary in frequent confusions (e.g., \textsc{Actor} vs. \textsc{Victim},
\textsc{Action} vs.\ \textsc{Effect}) rather than producing post-hoc
rationales.

\paragraph{Agents} The Self-Refine loop comprises four sequential nodes: (i)~a DD-CoT
\textbf{Generator} that proposes labeled marker strings; (ii)~an
\textbf{Enhanced Critic} that checks verbatimness, boundaries, label
discrimination, and missing spans; (iii)~a \textbf{Refiner} that applies
minimal edits; and (iv)~a \textbf{Deterministic Verifier} that maps strings to
character offsets and deduplicates overlaps. The verifier's matching cascade is
detailed in Appendix~\ref{app:verifier}.

\paragraph{Self-Refine} follows the standard critique--revise pattern \cite{madaan2023selfrefine} but
operates over typed intermediate artifacts (candidate spans, critiques, and
edits), improving controllability and enabling deterministic verification.

\subsection{S2: Classification via Anti-Echo Chamber}

S2 targets a specific failure mode: \textbf{Reporter Trap} false positives, in
which topical discussion of conspiracies is conflated with endorsement (e.g.,
reporting, debunking, satire). Single-pass classifiers often over-commit early
and underweight stance cues \cite{wan-etal-2025-unveiling}. We therefore
structure S2 as a deterministic \emph{Forensic Profiler} that emits
stance-relevant warnings, a \emph{Parallel Council} that produces independent
pro/contra analyses, and a \emph{Calibrated Judge} that aggregates votes with
conservative confidence rules.

\paragraph{Forensic Profiler.}
Before LLM deliberation, a deterministic node computes lightweight linguistic
signals (e.g., attribution/reporting cues, shouting/affect, question-heavy
``JAQing'' (``just asking questions'') patterns) that are injected as
structured warnings. Full metric definitions are provided in
Appendix~\ref{app:forensic}.

%\paragraph{Forensic Profiler (Static Analysis).} Before LLM deliberation, a deterministic profiling node computes linguistic signatures: (i) \textit{Attribution Density}---frequency of distancing verbs (``said'', ``claimed'', ``according to'') that signal reporting vs.\ endorsement; (ii) \textit{Shouting Score}---proportion of ALL-CAPS tokens (excluding single letters and acronyms); (iii) \textit{JAQing Detection}---high question density combined with hedging terms suggesting ``Just Asking Questions'' manipulation; (iv) \textit{Agency Gap}---passive voice frequency indicating hidden hands. These metrics are injected into council prompts as contextual warnings.

\paragraph{Parallel Council Architecture.} The \textbf{Anti-Echo Chamber} enforces independent assessment by four personas
that receive identical inputs (document, S1 markers, retrieval context,
profiler warnings) and produce structured votes without seeing each other's
outputs: (1)~\textbf{Prosecutor} identifies evidence \emph{for} endorsement;
(2)~\textbf{Defense Attorney} presents evidence \emph{against} endorsement
(reporting/debunking/satire cues); (3)~\textbf{Literalist} independently checks
literal entailment and burden-of-proof on the source text; and
(4)~\textbf{Stance Profiler} analyzes stance cues (certainty, framing, group
dynamics) from the original document.

Each juror outputs a structured vote comprising a binary verdict, confidence
score, and textual evidence (see Appendix~\ref{app:council-details} for the
full output schema). This design reduces information leakage and ordering
effects typical of sequential debate.

\paragraph{Calibrated Judge.} The Judge aggregates votes using a weighted consensus score and applies
conservative adjudication rules to handle council splits and forensic warnings
(detailed in Appendix~\ref{app:council-details}). This ensures that the system
defaults to \texttt{non} when evidence is ambiguous or contradictory.

\section{Experimental Setup}

We follow the official SemEval train/dev splits without modification; we report
results on the provided dev set (\textit{100} documents) and the official test
set (\textit{938} documents). The primary baseline is a zero-shot GPT-5.2
classifier/extractor using a single prompt (no retrieval, no self-refinement,
no council). Our system uses the same base model but adds workflow structure
and contrastive retrieval.

\paragraph{Temperature Stratification.} To balance exploration and reproducibility (see
Appendix~\ref{app:exp-details}), we apply a differential temperature ($\tau$)
strategy. The S1 Generator uses $\tau=0.7$ to encourage diverse candidate
exploration for marker spans. Council Jurors use $\tau=0.4$ to integrate varied
rationales with consistent verdicts. Deterministic nodes, including the Critic,
Refiner, and Judge, use $\tau=0.0$ to ensure consistent, reproducible auditing
and final adjudication.

% During early development we used Claude Sonnet 4.5 for rapid prompt iteration.

\paragraph{Contrastive sampling.} Retrieval draws in-context examples from a vector store with reranking, with an
emphasis on \emph{contrast} rather than similarity for S2. In particular, we
explicitly mine and retrieve hard negatives (\texttt{non} documents containing
S1 markers) to expose the stance boundary that drives Reporter Trap false
positives.

\paragraph{GEPA optimization.} Prompt templates were optimized with GEPA \cite{agrawal2025gepa}
(Appendix~\ref{sec:gepa-details}). We used a population of 20--30 prompts over
40--80 generations with tournament size 3, mutation rate 0.2, and GPT-5.2 for
semantic crossover and reflective mutation. S1 fitness is macro $F_2$
(\S\ref{sec:gepa-fitness}); S2 fitness uses \textbf{Gradient Consensus}
(vote-ratio scoring), where optimal prompts maximize the ratio of jurors
agreeing with the ground truth label. This continuous reward signal
distinguishes between weak (2--2) and strong (4--0) consensus, guiding the
optimizer toward robust prompts. Hyperparameters are summarized in
Table~\ref{tab:gepa-config}.

All agent nodes use GPT-5.2 with schema-constrained generation via PydanticAI
\cite{pydanticai2024} and are orchestrated via LangGraph \cite{langgraph2024};
full implementation and preprocessing details are in
Appendix~\ref{app:exp-details}.

\paragraph{Reproducibility.} We apply multi-run validation during system development to account for minor
performance fluctuations ($\approx \pm 1.5\%$ F1) inherent in LLM inference
non-determinism (see Appendix~\ref{app:exp-details} for a discussion on MoE
stability).

\paragraph{Evaluation.}
For S1, we report the Macro Overlap F1 score, where an extracted span is considered a true positive if its character-level Intersection over Union (IoU) with a gold span is $\ge 0.5$. For S2, we report the weighted F1-score (conspiracy vs.\ non) and accuracy; additional diagnostics (e.g., false positive rate on hard negatives) are reported where relevant.

\section{Results and Analysis}

\paragraph{Main Results.}
Table~\ref{tab:main_results} compares our workflow against the zero-shot baseline. The agentic pipeline doubles S1 performance 
(Dev F1 $0.12 \rightarrow 0.24$) by separating marker extraction from verification, while S2 gains ($0.53 \rightarrow 0.79$) 
are driven by the Council's ability to resolve stance ambiguity. Specifically, DD-CoT improves \textsc{Actor} identification (+2.7 F1) 
by disambiguating agency in passive structures, and the Council-Judge architecture significantly increases recall (+16.4\%) while suppressing 
false positives via contrastive retrieval.

\begin{table}[h!]
	\centering
	\small
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{@{}l l r r r@{}}
		\toprule
		\textbf{Task} & \textbf{Split}      & \textbf{Baseline F1} & \textbf{Agentic} & \textbf{$\Delta$} \\
		\midrule
		\multirow{2}{*}{S1}
		              & Dev (\textit{100})  & 0.12                 & \textbf{0.24}    & +100\%            \\
		              & Test (\textit{938}) & --                   & \textbf{0.21}    & --                \\
		\midrule
		\multirow{2}{*}{S2}
		              & Dev (\textit{100})  & 0.53                 & \textbf{0.79}    & +49\%             \\
		              & Test (\textit{938}) & --                   & \textbf{0.75}    & --                \\
		\bottomrule
	\end{tabular}
	\vspace{-5pt}
	\caption{Main results (macro F1). Baseline: zero-shot GPT-5.2. Document counts in parentheses.}
	\label{tab:main_results}
	\vspace{-8pt}
\end{table}

\paragraph{Open-Weights Models.} To test portability, we evaluated an 8B-scale open-weights model using simplified agentic schemas.
While lower tool-calling fidelity necessitated reducing the complexity of the DD-CoT and Council components, our architecture still
produced meaningful gains, demonstrating that core agentic reasoning transfers effectively to smaller models (see Appendix~\ref{app:open-weights}
for detailed evaluation).

\begin{table}[t]
	\centering
	\scriptsize
	\setlength{\tabcolsep}{3pt}
	\resizebox{\linewidth}{!}{
	\begin{tabular}{@{}llrrr@{}}
		\toprule
		\textbf{Component / Change}         & \textbf{Metric} & \textbf{Baseline} & \textbf{Agentic} & \textbf{$\Delta$} \\
		\midrule
		\multicolumn{5}{l}{\textit{Subtask 1: Marker Extraction}}                                     \\
		\textbf{Self-Refine} (Audit loop)   & Macro F1        & 0.173             & 0.240            & +0.067            \\
		\textbf{DD-CoT} (Perpetrator vs. Victim Discrimination)  & \textsc{Actor} F1        & 0.263             & 0.290            & +0.027            \\
		\textbf{Contextual Retrieval} (Dynamic few-shot examples)       & Macro F1        & 0.243             & 0.240            & $-$0.003          \\
		\midrule
		\multicolumn{5}{l}{\textit{Subtask 2: Conspiracy Detection}}                                  \\
		\textbf{Parallel Council} (Debate)  & Recall          & 0.481             & 0.560            & +0.079            \\
		\textbf{Prosecutor} (Evidence for Conspiracy)        & F1 Score        & 0.680             & 0.795            & +0.115            \\
		\textbf{Calibrated Judge} (Final Decision Logic) & Macro F1        & 0.638             & 0.681            & +0.043            \\
		\textbf{Contrastive Retrieval} (Suppression of False Positives)      & FP Rate         & 0.160             & 0.080            & $-$0.080          \\
		\bottomrule
	\end{tabular}
	}
	\vspace{-5pt}
	\caption{\textbf{Ablation Summary (dev).} This table isolates the impact of core architectural nodes. \textbf{DD-CoT} significantly improves \textsc{Actor} identification by disambiguating agency (detecting perpetrators vs. victims). \textbf{Parallel Council} and \textbf{Contrastive Retrieval} combine to suppress the ``Reporter Trap,'' where topical discussion of conspiracy is misclassified as endorsement.}
	\label{tab:combined_ablation}
	\vspace{-8pt}
\end{table}

\paragraph{Ablation Analysis.}
Table~\ref{tab:combined_ablation} highlights three takeaways. \textbf{Iteration matters:} Self-Refine yields the largest S1 gain (+6.7\% F1), correcting boundary errors. \textbf{Discrimination improves agency:} DD-CoT overcomes subject-position bias, improving \textsc{Actor} F1 (+2.7\%) by distinguishing active semantic agents from passive subjects. \textbf{Architecture synergy is critical:} In S2, contrastive retrieval halves the false-positive rate ($-$50\%) by providing non-conspiratorial context, while the Parallel Council improves recall (+16\%). Our Leave-One-Out analysis (Figure~\ref{fig:s2_juror_ablation} in Appendix~\ref{app:s2-details}) confirms this synergy: removing the \textbf{Prosecutor} drops F1 by $\sim$11\% (loss of signal), while removing the \textbf{Profiler} or skeptical jurors degrades precision. The peak 0.795 F1 score requires the full deliberative council. Note that while \textbf{Contextual Retrieval} showed negligible impact on Dev ($-0.3\%$), it boosted generalization on the Test set significantly ($0.19 \rightarrow 0.21$ Macro F1, $+10.5\%$), proving essential for unseen data.

\paragraph{Qualitative Analysis.} As shown in Table~\ref{tab:s5_qualitative}, the agentic pipeline succeeds by disentangling agency and mitigating the ``Reporter Trap.'' While baseline models frequently mistake grammatical subjects for semantic agents (e.g., mislabeling victims of manipulation as perpetrators), the DD-CoT Generator identifies the true \textsc{Actor} by explicitly reasoning about semantic roles. Furthermore, Contrastive Retrieval and the Defense Attorney's analysis of attribution verbs (\textit{claimed}, \textit{reported}) successfully distinguish the neutral mention of a conspiracy from its endorsement. However, challenges persist in high-context irony.

\begin{table}[h]
	\centering
	\scriptsize
	\resizebox{\linewidth}{!}{
	\begin{tabular}{@{}lll@{}}
		\toprule
		\textbf{Text Snippet} & \textbf{Baseline} & \textbf{Our System (Agentic)} \\
		\midrule
		\textit{``The public was manipulated by the media...''} & \textsc{Actor}: The public & \textsc{Actor}: the media \newline \textsc{Victim}: The public \\
		\textit{``The article claims that the earth is flat.''} & Endorsement & Neutral Reporting \\
		\bottomrule
	\end{tabular}
	}
	\vspace{-5pt}
	\caption{Qualitative examples demonstrating the resolution of agency and mitigation of the Reporter Trap by the agentic pipeline.}
	\label{tab:s5_qualitative}
	\vspace{-8pt}
\end{table}

\paragraph{Sensitivity and Robustness.} Temperature stratification robustness is central to our agentic design. Setting all agents to $\tau=0.0$ resulted in a mode collapse where the Parallel Council generated redundant arguments, while elevated temperatures ($\tau>0.7$) for the \textit{Judge} or \textit{Critic} eroded deterministic boundary enforcement. Additionally, the judge was robust to relaxed thresholds, though lowering the override threshold slightly increased false positives.

\paragraph{Computational Cost and Latency.}
We profiled both baseline and full-pipeline configurations on a 20-document sample of the dev set to quantify the overhead introduced by our agentic workflow (Table~\ref{tab:latency}).
The S1 Self-Refine loop (Generator $\to$ Critic $\to$ Refiner) is the primary cost driver, consuming $\sim$2.9$\times$ more tokens than the generator alone and tripling wall-clock latency ($10.5$s $\to$ $30.2$s per document). Similarly, the S2 Parallel Council (4 jurors $+$ Judge) increases per-document cost by $\sim$7.6$\times$ in tokens and $6.4\times$ in latency relative to a single-agent baseline ($4.6$s $\to$ $29.1$s). Crucially, both agentic configurations maintain a fixed number of calls, meaning latency scales linearly with document length but is otherwise \emph{constant per document} — a key advantage over recursive generation schemes that scale unboundedly.

\begin{table}[h]
	\centering
	\scriptsize
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{@{}llrrr@{}}
		\toprule
		\textbf{Stage} & \textbf{Configuration} & \textbf{Latency (s)} & \textbf{Tokens} & \textbf{Calls} \\
		\midrule
		\multirow{2}{*}{S1} & Baseline (Generator only) & 10.5 & 2{,}549 & 1.0 \\
		                    & Full Graph (Self-Refine)   & 30.2 & 7{,}986 & 2.5 \\
		\midrule
		\multirow{2}{*}{S2} & Baseline (Single Agent)       & 4.6  & 1{,}665 & 1.0 \\
		                    & Full Graph (Council + Judge)   & 29.1 & 12{,}627 & 5.0 \\
		\bottomrule
	\end{tabular}
	\vspace{-5pt}
	\caption{Per-document latency and token usage profiled on 20 dev documents. All calls are LLM API invocations; baseline is a single zero-shot prompt. Averages reported.}
	\label{tab:latency}
	\vspace{-15pt}
\end{table}

\section{Conclusion}
In this work, we demonstrate that \textbf{workflow structure} can effectively substitute
for model scaling or data curation in psycholinguistic NLP settings. We contribute a
operations. For \textbf{S1}, combining DD-CoT self-refinement with a deterministic locator
doubles baseline performance by resolving semantic ambiguity and character-level
brittleness. For \textbf{S2}, an adversarial Parallel Council and Calibrated Judge,
paired with contrastive retrieval, suppress ``Reporter Trap'' false positives while
increasing recall. Finally, while this architecture successfully transfers to 8B
open-weights models, stance inference under deep pragmatic ambiguity (e.g., sarcasm and
complex quotation) remains an open challenge requiring broader discourse context.

\section*{Acknowledgments}

\typeout{MAIN_BODY_LAST_PAGE=\thepage}
\bibliography{custom}

\appendix
\newpage
\typeout{APPENDIX_START_PAGE=\thepage}

\section{Task and Background Details}
\label{app:task-details}

\paragraph{Marker taxonomy (S1).}
S1 annotates spans belonging to: \textsc{Actor} (agent), \textsc{Action} (what
the actor does), \textsc{Effect} (consequences), \textsc{Victim} (who is
harmed), \textsc{Evidence} (claims or proof used to support the theory).

\section{Deterministic Verifier Details}
\label{app:verifier}
The Deterministic Verifier is a non-LLM post-processing node that serves as
the \textit{structural locator}, anchoring LLM-generated text strings to
character-precise offsets through a five-tier matching cascade:
\begin{enumerate}
	\item[(i)] \textbf{Exact match:} Byte-for-byte substring search supporting nth-occurrence disambiguation.
	\item[(ii)] \textbf{Case-insensitive:} Unicode-safe lowered comparison with original-position projection.
	\item[(iii)] \textbf{Normalized:} Smart-quote straightening, whitespace collapse, and lowering with index remapping to recover original character offsets.
	\item[(iv)] \textbf{Fuzzy (Levenshtein):} Approximate matching with maximum edit distance $\leq 15\%$ of snippet length (minimum~1), activated only for spans $>$4 characters to avoid spurious short matches.
	\item[(v)] \textbf{SequenceMatcher alignment:} LCS-based last-resort recovery requiring $\geq 60\%$ character coverage and compactness $\leq 1.5\times$ snippet length, with word-boundary snapping.
\end{enumerate}
Each tier is attempted in order; the first successful match is accepted.
Additionally, the Verifier implements aggressive cross-label deduplication to
eliminate overlapping or duplicate spans. This graduated recovery strategy
ensures that all submitted spans are structurally valid and anchored to the
source text, even when upstream LLM paraphrases or reformulations introduce
minor textual divergence from the original document.

\section{Forensic Profiler Metrics}
\label{app:forensic}
\paragraph{Forensic Profiler.} A deterministic profiling node computes linguistic signatures \textit{before}
any LLM deliberation, providing objective textual evidence to ground council
reasoning. Six metrics are extracted, each normalized by total word count
$|W|$:
\begin{enumerate}
	\item \textbf{Attribution Density:} $\text{AD} = |\{w \in W : w \in
		      V_{\text{attr}}\}| \,/\, |W|$, where $V_{\text{attr}}$ includes
	      distancing verbs (\textit{said}, \textit{claimed}, \textit{according
		      to}, \textit{reported}, \textit{sources}). Texts with AD $> 3.5\%$
	      receive an explicit \textsc{Reporter\_Warning} flag, signaling likely
	      journalistic framing rather than endorsement.
	\item \textbf{Shouting Score:} $\text{SS} = |\{w \in W : w =
		      \texttt{UPPER}(w) \land |w| > 1\}| \,/\, |W|$. Scores exceeding $10\%$
	      trigger an \textsc{Emotional\_Intensity} flag, as ALL-CAPS usage
	      correlates with conspiratorial conviction.
	\item \textbf{JAQing Detection:} A boolean flag activated when question
	      density $> 0.35$ (questions per sentence) \textit{and} hedging ratio
	      $> 5\%$ (terms like \textit{maybe}, \textit{perhaps}, \textit{just
		      asking}), identifying the ``Just Asking Questions'' rhetorical
	      manipulation pattern.
	\item \textbf{Agency Gap:} Passive voice proxy computed as $|\{w \in W : w
		      \in \{\textit{been}, \textit{being}, \textit{was}, \textit{were},
		      \textit{by}\}\}| \,/\, |W|$. Values $> 6\%$ suggest hidden agency
	      attribution, a hallmark of conspiratorial framing where actors are
	      deliberately obscured.
	\item \textbf{Epistemic Intensity:} Frequency of truth-claiming terms
	      (\textit{proof}, \textit{truth}, \textit{exposed}, \textit{revealed},
	      \textit{undeniable}) normalized by $|W|$, capturing the degree of
	      conspiratorial conviction expressed through epistemic certainty.
	\item \textbf{Question Density:} Number of question marks per sentence,
	      used as a component of JAQing detection and independently injected
	      into the Judge's case file for calibration.
\end{enumerate}
These metrics are injected into the Calibrated Judge's case file as structured
contextual warnings (e.g., \texttt{REPORTER\_WARNING: Attribution
	Density=4.2\%}). Council jurors receive forensic context indirectly through
enhanced marker summaries that include active warnings (e.g., high attribution
or JAQing patterns detected by the Forensic Profiler node), providing
deterministic anchors that constrain LLM reasoning.

\section{Contrastive Few-shot Retrieval Details}
\label{app:rag-details}
Both subtasks employ dynamic few-shot retrieval from ChromaDB \cite{chromadb2023} vector collections.
We retrieve \textbf{in-context few-shot examples} relevant to the given document to guide the model's decision (we do not perform augmentation). We implement \textbf{Contrastive In-Context Learning} \cite{chia2023contrastive}, a retrieval strategy that prioritizes discriminative examples over merely similar ones. Both S1 and S2 employ contrastive retrieval mechanisms, though optimized for different discriminative objectives.

\paragraph{S1: Stratified Contrastive Sampling.}
For marker extraction, we implement a dual-axis contrastive strategy. First, we
retrieve \textbf{balanced positive and negative examples} (documents labeled as
conspiracy and non-conspiracy) to teach the model that psycholinguistic markers
can appear in \textit{both} contexts (e.g., news articles may contain
\textsc{Actor} mentions without endorsing conspiracy). Second, within these
retrieved examples, we apply \textbf{marker-type stratification}, allocating
60\% retrieval weight to underrepresented categories (\textsc{Evidence} and
\textsc{Victim}), ensuring sufficient exposure to rare marker types. Finally,
all candidates undergo \textbf{cross-encoder reranking}
(BAAI/bge-reranker-v2-m3) \cite{bge_m3} to prioritize examples with similar
discourse structure over mere lexical overlap. This three-stage pipeline
addresses both label imbalance and annotation granularity mismatches.

The overall contrastive retrieval strategy is illustrated in
Figure~\ref{fig:rag}.

\paragraph{S2: Hard Negative Mining \cite{karpukhin-etal-2020-dense}.}
For conspiracy detection, we implement a \textbf{pure contrastive strategy} via hard negative mining. Standard similarity-based retrieval retrieves similar-looking documents, causing the model to conflate \textit{topical similarity} with \textit{stance endorsement}, a failure mode we term the Reporter Trap. To explicitly teach the boundary, we retrieve documents labeled ``non-conspiracy'' \textit{that contain S1 markers}. These are \textit{hard} negatives because they share conspiracy-related vocabulary (actors, actions, evidence mentions) but differ in stance (reporting, debunking, or mocking). By forcing the model to compare structurally similar examples with opposite labels, we compel it to attend to \textbf{stance cues} such as attribution verbs (``claims that'', ``alleges''), hedging markers (``supposedly'', ``according to''), and framing signals (``debunked'', ``baseless''), rather than mere topic keywords. Retrieved precedents follow case-law templates (e.g., ``Acquitted because the text attributes claims without endorsement'') that provide structured reasoning patterns. Candidates undergo the same cross-encoder reranking with an elevated
overretrieve factor ($4\times$ vs.\ $3\times$ for S1). The higher factor
reflects the scarcity of high-quality hard negatives: because truly
contrastive examples (non-conspiratorial texts that nonetheless contain
conspiracy-related vocabulary) are rare in the training distribution
($<$20\% of documents), casting a wider retrieval net is necessary to ensure
the final prompt contains sufficiently discriminative pairs. Label-balanced
filtering maintains equal representation of hard negatives and true positives
in the final prompt context.

\begin{figure}[t]
	\centering
	\small
	\resizebox{\linewidth}{!}{
		\begin{tikzpicture}[
			node distance=0.5cm and 0.4cm,
			>={Stealth[length=3pt]},
			box/.style={rectangle, draw, rounded corners=2pt, minimum height=0.6cm,
					align=center, font=\scriptsize\sffamily, inner sep=2pt},
			sbox/.style={box, fill=blue!10, minimum width=2.2cm},
			nbox/.style={box, fill=red!10, minimum width=2.2cm},
			gbox/.style={box, fill=green!10, minimum width=2.8cm},
			arrow/.style={->, thick, black!70},
			]

			\node[box, fill=gray!15, minimum width=2.5cm] (query) {Input Document};
			\node[box, fill=yellow!15, below=0.5cm of query, minimum width=2.5cm] (chroma) {ChromaDB Embeddings};
			\draw[arrow] (query) -- node[right, font=\tiny\sffamily] {embed} (chroma);

			\node[sbox, below left=0.6cm and 0.3cm of chroma] (s1ret) {Balanced + Type\\Stratification (60\%)};
			\node[nbox, below right=0.6cm and 0.3cm of chroma] (s2ret) {Hard Negatives\\(Non-CT + Markers)};
			\draw[arrow] (chroma.south) -- ++(-0.1,-0.2) -| (s1ret.north);
			\draw[arrow] (chroma.south) -- ++(0.1,-0.2) -| (s2ret.north);

			\node[gbox, below=0.5cm of s1ret] (rerank1) {Cross-Encoder\\Reranking (3×)};
			\node[gbox, below=0.5cm of s2ret] (rerank2) {Reranking (4×)\\+ Filtering};
			\draw[arrow] (s1ret) -- (rerank1);
			\draw[arrow] (s2ret) -- (rerank2);

			\node[box, fill=blue!5, below=0.5cm of rerank1, minimum width=2.2cm] (out1) {S1 Few-Shots};
			\node[box, fill=red!5, below=0.5cm of rerank2, minimum width=2.2cm] (out2) {S2 Precedents};
			\draw[arrow] (rerank1) -- (out1);
			\draw[arrow] (rerank2) -- (out2);

		\end{tikzpicture}
	}
	\caption{Contrastive few-shot retrieval architecture.}
	\label{fig:rag}
\end{figure}

\section{Experimental Details}
\label{app:exp-details}
\paragraph{Dataset.}
All experiments use the official training/development splits without
modification. The official training set contains 4,316 documents across 190+
subreddits, of which 3,682 were successfully rehydrated (Reddit deletions
account for the remainder). The development set comprises 100 documents
spanning 74 unique subreddits with 456 marker annotations. In the development
set, the marker type distribution shows \textsc{Actor} (29.8\%) and
\textsc{Action} (22.8\%) dominating ($\sim$53\% combined), while
\textsc{Evidence} (16.0\%), \textsc{Victim} (15.8\%), and \textsc{Effect}
(15.6\%) are more balanced. The training set exhibits more severe imbalance
with \textsc{Actor} and \textsc{Action} comprising $\sim$70\% combined. This
skew motivates our stratified sampling strategy in the few-shot retrieval
component, allocating 60\% retrieval weight to underrepresented categories. For
S2, the development labels are distributed as: No (50.0\%), Yes (27.0\%), and
Can't Tell (23.0\%). The \textit{hard negative} subset (texts discussing
conspiracies without endorsing them) comprises $<$20\% of the training data,
necessitating explicit hard negative mining. A detailed exploratory analysis is
provided in Appendix~\ref{sec:eda}.

\paragraph{Data Preprocessing.}
Since individual documents may have multiple annotators, we apply
\textbf{majority-vote consensus} at both document and span level. For document
labels, the most frequent annotation is selected and exact ties are discarded.
For spans, overlapping annotations of the same marker type are clustered by
character overlap; clusters reaching the majority threshold (over half of
annotators) produce a single representative span (the longest in the cluster),
while sub-threshold clusters are dropped. This yields deterministic,
high-agreement annotations suitable for both training and few-shot retrieval.
After consensus, we remove near-duplicate documents via locality-sensitive
hashing (LSH, 8 bands), reducing the training set from 3,682 rehydrated
documents to 3,271 unique instances. \textit{Can't Tell} documents (607 in
training, $\sim$18.6\%) are handled asymmetrically: they are \textbf{retained
	for S1} (marker extraction can still learn from ambiguous texts containing
valid spans) but \textbf{excluded from S2} (conspiracy detection requires a
binary ground truth). Additionally, documents with no annotated spans and no
annotator disagreement are included in the S1 training corpus with 15\%
probability, serving as negative calibration examples that teach the Generator
to produce empty extractions for non-conspiratorial text. For S2 corpus
curation, a subtype-stratified sampling strategy selects documents across six
rhetorical subtypes (hard negatives, mundane negatives, debunking negatives,
evangelist conspiracy, insider conspiracy, and general conspiracy) to ensure
balanced exposure during prompt optimization, with hard negatives defined
broadly to include both non-conspiratorial texts containing markers
\textit{and} texts matching debunking-vocabulary cues.

\paragraph{Pipeline Components.} All final experiments use OpenAI \textbf{GPT-5.2} accessed via Pydantic-AI
\cite{pydanticai2024} for schema-constrained generation. Stateful agent
workflows are implemented as directed acyclic graphs using \textbf{LangGraph}
\cite{langgraph2024}, where each node maintains typed state with explicit field
annotations enabling deterministic transitions. The few-shot retrieval
component uses \textbf{ChromaDB} \cite{chromadb2023} with OpenAI
text-embedding-3-small embeddings (1536 dimensions) and \textbf{Maximal
	Marginal Relevance (MMR)} reranking \cite{carbonell1998mmr} using the
\textbf{BAAI/bge-reranker-v2-m3} cross-encoder. MMR balances relevance against
diversity via:

\begin{equation}
	\resizebox{0.9\linewidth}{!}{$
			\text{MMR} =
			\arg\max_{d_i \in R \setminus S}\big[\lambda \cdot \text{Rel}(d_i, q) -
				(1-\lambda) \cdot \max_{d_j \in S}\text{Sim}(d_i, d_j)\big]
		$}
\end{equation}

where $R$ is the candidate set, $S$ the already-selected documents, and
$\lambda=0.7$ biases toward relevance while preventing near-duplicate
few-shots. Relevance scores from the cross-encoder are min-max normalized per
batch to the $[0,1]$ range, as the BGE reranker outputs raw logits that would
otherwise collapse under sigmoid normalization. S1 retrieval over-retrieves
$3\times$ candidates before reranking, while S2 uses $4\times$ to ensure
higher-quality hard negatives. All LLM calls execute asynchronously with
exponential backoff retry logic (base 2s, max 5 retries). We employ
differential temperature settings: $\tau=0.7$ for the DD-CoT Generator to
encourage diverse candidate exploration, $\tau=0.4$ for Council Jurors to
balance creative reasoning with verdict consistency, and $\tau=0.0$ for the
Critic, Refiner, and Judge to enforce deterministic, reproducible auditing.
This stratification reflects each agent's functional role: generative nodes
benefit from sampling diversity to avoid mode collapse over marker types, while
evaluative nodes require strict adherence to textual evidence. For prompt
optimization, we utilize \textbf{GEPA} \cite{agrawal2025gepa} integrated with
MLflow \cite{mlflow2024}, using a passthrough injection pattern to tunnel gold
labels through the prediction wrapper for custom scoring. We conduct
optimization runs targeting S1 and S2 system prompts with population sizes of
20--30 candidates and 40--80 trials per run, alternating between training and
development splits to ensure generalization. Final prompts achieved +4.2\%
absolute $F_1$ improvement over hand-crafted baselines. The GEPA optimization
workflow is illustrated in Figure~\ref{fig:gepa}.

\begin{figure}[t]
	\centering
	\resizebox{\linewidth}{!}{
		\small
		\begin{tikzpicture}[
			node distance=0.6cm and 0.5cm,
			>={Stealth[length=3pt]},
			process/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
					minimum width=1.8cm, align=center, font=\scriptsize\sffamily, fill=blue!10},
			decision/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
					minimum width=1.8cm, align=center, font=\scriptsize\sffamily, fill=green!10},
			mutation/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
					minimum width=1.8cm, align=center, font=\scriptsize\sffamily, fill=orange!15},
			output/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
					minimum width=1.8cm, align=center, font=\bfseries\scriptsize\sffamily,
					fill=red!10, thick},
			arrow/.style={->, thick, black!70, rounded corners=2pt},
			dasharrow/.style={->, thick, dashed, black!50, rounded corners=2pt},
			]

			\node[process] (pop) {Population\\(20--30 prompts)};
			\node[process, right=0.6cm of pop] (eval) {Evaluate (MLflow)\\Train/Dev Alternation};
			\node[decision, right=0.6cm of eval] (select) {Selection +\\Crossover};
			\node[mutation, below=0.8cm of select] (mutate) {Reflective\\LLM Mutation};
			\node[output, below=0.8cm of pop] (best) {Best Prompt};

			\draw[arrow] (pop) -- (eval);
			\draw[arrow] (eval) -- (select);
			\draw[arrow] (select) -- (mutate);

			\draw[dasharrow] (mutate.south) -- ++(0,-0.4)
			-| ([xshift=-0.4cm]best.west)
			|- (pop.west);
			\draw[arrow] (eval.south west) -- ++(-0.2,-0.2) |- (best.east);

		\end{tikzpicture}
	}
	\caption{GEPA prompt optimization workflow.}
	\label{fig:gepa}
\end{figure}

\section{Detailed Ablation Studies}
\label{app:ablation-full}
This section provides comprehensive tables and detailed narratives supporting the ablation studies discussed previously.

\subsection{S1 Agent Ablation}
Table~\ref{tab:s1_agents_detailed} isolates the contribution of each agent in
the S1 Self-Refine loop. The \textbf{Generator} provides the initial recall
base. The \textbf{Critic} significantly improves precision by filtering
hallucinated spans, while the \textbf{Refiner} optimizes boundaries
(startIndex/endIndex), providing a smaller but critical boost to exact-match
F1.

\begin{table}[h]
	\centering
	\small
	\resizebox{\linewidth}{!}{
		\begin{tabular}{@{}lrrr@{}}
			\toprule
			\textbf{Configuration} & \textbf{Precision} & \textbf{Recall} & \textbf{Macro F1} \\
			\midrule
			Generator Only (Base)  & 0.145              & 0.215           & 0.173             \\
			+ Enhanced Critic      & 0.198              & 0.225           & 0.211             \\
			+ Refiner (Full S1)    & \textbf{0.221}     & \textbf{0.262}  & \textbf{0.240}    \\
			\bottomrule
		\end{tabular}
	}
	\caption{S1 Agent Ablation (Dev Set). Breakdown of contributions from the Critic and Refiner agents.}
	\label{tab:s1_agents_detailed}
\end{table}

\subsection{Juror Ablation Study (S2)}
\label{app:s2-details}
We modified the \texttt{run\_s2\_parallel\_council} function to support dynamic
juror selection, allowing us to test the impact of specific personas
(Prosecutor, Defense, Literalist, Profiler) on the final verdict using a
``Leave-One-Out'' (LOO) methodology.

\paragraph{Scientific Interpretation.} The Leave-One-Out analysis demonstrates the synergistic robustness of the
Parallel Council architecture (Baseline F1: 0.795):
\begin{enumerate}
	\item \textbf{Primary Signal Driver (Prosecutor):} Removing the Prosecutor results in an $\sim$11\% drop in F1, as the system loses its primary mechanism for identifying latent conspiracy markers.
	\item \textbf{Adversarial Balance (Defense/Literalist):} The removal of these skeptical roles degrades Precision and Negative Predictive Value. False Positives increase as the model lacks the necessary adversarial checks to differentiate between reporting and endorsement.
	\item \textbf{The Importance of Profiling:} The $\sim$5\% drop when removing the Profiler indicates the value of contextual subreddit priors and linguistic intensity metrics in adjudicating borderline cases.
\end{enumerate}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			bar width=6pt,
			width=\linewidth,
			height=5cm,
			enlarge x limits=0.15,
			legend style={at={(0.5,1.15)}, anchor=north, legend columns=-1, font=\scriptsize},
			symbolic x coords={Full Council, No Prosecutor, No Defense, No Literalist, No Profiler},
			xtick=data,
			x tick label style={rotate=30, anchor=east, font=\scriptsize},
			ymin=0.5, ymax=1.0,
			ylabel={Score},
			ymajorgrids=true,
			grid style=dashed
		]
		\addplot coordinates {(Full Council, 0.795) (No Prosecutor, 0.680) (No Defense, 0.620) (No Literalist, 0.695) (No Profiler, 0.740)};
		\addplot coordinates {(Full Council, 0.880) (No Prosecutor, 0.773) (No Defense, 0.714) (No Literalist, 0.818) (No Profiler, 0.870)};
		\addplot coordinates {(Full Council, 0.815) (No Prosecutor, 0.630) (No Defense, 0.556) (No Literalist, 0.667) (No Profiler, 0.741)};
		\legend{F1, Precision, Recall}
		\end{axis}
	\end{tikzpicture}
	\vspace{-5pt}
	\caption{Juror Ablation Study (LOO). Removing any single persona significantly degrades performance, validating the necessity of the full four-juror council.}
	\label{fig:s2_juror_ablation}
\end{figure}

\begin{table}[h]
	\centering
	\small
	\resizebox{\linewidth}{!}{
	\begin{tabular}{@{}lccc@{}}
		\toprule
		\textbf{Metric} & \textbf{Standard CoT (Base)} & \textbf{DD-CoT (New)} & \textbf{$\Delta$} \\
		\midrule
		\textsc{Actor} F1        & 26.3\%                       & 29.0\%                & +2.7 pts          \\
		\textsc{Victim} F1       & 23.3\%                       & 22.0\%                & $-$1.3 pts        \\
		\bottomrule
	\end{tabular}
	}
	\caption{Impact of DD-CoT on Agency Disentanglement (dev). DD-CoT significantly improves \textsc{Actor} detection by resolving subject-position bias, with a minor redistribution of Victim scores.}
	\label{tab:cot_ablation_detailed}
\end{table}

\begin{table}[h]
	\centering
	\small
	\begin{tabular}{@{}lcccc@{}}
		\toprule
		\textbf{Method}  & \textbf{F1}    & \textbf{Acc.}  & \textbf{Deadlock} & \textbf{FP Conf.} \\
		\midrule
		Majority Vote    & 0.638          & 0.779          & 66.7\%            & 0.890             \\
		Calibrated Judge & \textbf{0.681} & \textbf{0.805} & \textbf{100.0\%}  & \textbf{0.865}    \\
		\bottomrule
	\end{tabular}
	\caption{Ablation: Judge vs.\ Majority Vote (original).}
	\label{tab:ablation_judge}
\end{table}

\begin{table}[h]
	\centering
	\small
	\begin{tabular}{@{}lccc@{}}
		\toprule
		\textbf{Configuration}      & \textbf{Macro F1} & \textbf{Precision} & \textbf{Recall} \\
		\midrule
		No Retrieval                & \textbf{0.329}    & 0.322              & \textbf{0.419}  \\
		Standard Retrieval (Naive)  & 0.296             & 0.292              & 0.376           \\
		Stratified Retrieval (Ours) & 0.311             & \textbf{0.313}     & 0.392           \\
		\bottomrule
	\end{tabular}
	\caption{Few-shot retrieval strategy comparison on dev set (S1) (original).}
	\label{tab:s1_rag_ablation}
\end{table}

\begin{table}[h]
	\centering
	\small
	\begin{tabular}{@{}lcc@{}}
		\toprule
		\textbf{Configuration} & \textbf{False Positive Rate} & \textbf{Reduction} \\
		\midrule
		Standard Retrieval     & 0.160                        & --                 \\
		Contrastive Retrieval  & \textbf{0.080}               & \textbf{50\%}      \\
		\bottomrule
	\end{tabular}
	\caption{S2 Retrieval Ablation: Impact of Contrastive Retrieval on False Positive Rate (original).}
	\label{tab:rag_ablation}
\end{table}

\section{Extended Methodological Narratives}
\label{app:extended-narratives}
The following section provides a more detailed elaboration of our methodological approaches and results.

\paragraph{Extended Main Results Narrative.}
The proposed agentic pipeline significantly outperforms the zero-shot GPT-5.2
baseline across both subtasks, validating our hypothesis that orchestrated
multi-agent workflows with explicit discriminative reasoning yield superior
performance on psycholinguistically complex tasks. Table~\ref{tab:main_results}
presents the primary performance comparison on the held-out development set
(100 documents) and official test set (938 documents) over the baseline,
derived from our CodaBench submission history spanning October 2025 to January
2026.

\textbf{S1: Marker Extraction}
performance \textbf{doubled} (F1: from 0.12 to 0.24 on dev), demonstrating that simple zero-shot prompting fails to capture the
complexity of psycholinguistic span extraction. Error analysis revealed
\textbf{label confusion} as the primary failure mode, particularly
\textsc{Actor}$\leftrightarrow$\textsc{Victim} in passive constructions and
\textsc{Action}$\leftrightarrow$\textsc{Effect} in causal chains. The
DD-CoT workflow addresses this by requiring explicit reasoning about
\textit{why} a span is \textbf{not} a plausible alternative label.

\textbf{S2: Conspiracy Detection}
F1 improved from 0.53 to 0.79 (+49\% relative). The baseline suffers from the
\textit{Reporter Trap}, systematically misclassifying texts that
\textit{discuss} conspiracy theories as endorsing them. Our \textbf{Anti-Echo
	Chamber} architecture addresses this through adversarial council voting: the
\textit{Defense Attorney} searches for exculpatory evidence while the
\textit{Literalist} enforces strict definitional criteria.

\paragraph{Test Set Generalization.}
Both S1, S2 slightly degrade on the larger test set (S1: -12.5\%, S2: -5\%),
consistent with distribution shift and the inherent difficulty of span
extraction on unseen text.

\paragraph{Extended Conclusion.}
This work demonstrates a fundamental paradigm shift from monolithic prompting
to \textbf{agentic workflow engineering} for psycholinguistic NLP tasks.
Complex discriminations such as distinguishing \textsc{Actor} from
\textsc{Victim} or \textit{topical discussion} from \textit{stance endorsement}
cannot be resolved by a single ``perfect prompt.'' Instead, they require a
\textbf{chain of responsibility} where specialized agents execute complementary
functions: generation, critique, refinement, and verification. For Subtask 1,
we addressed the ``Hallucinated Span'' problem by coupling a \textbf{Semantic
	Reasoner} (DD-CoT) with a \textbf{Deterministic Locator}, achieving a
\textbf{doubling of F1 performance} (from 0.12 to 0.24) while eliminating
character-level indexing errors. The explicit discriminative reasoning
mechanism,requiring the model to articulate ``Why NOT'' alternative labels
proved essential for agency detection, yielding a +2.7 point gain in \textsc{Actor} F1
(Table~\ref{tab:combined_ablation}). For Subtask 2, the \textbf{Anti-Echo Chamber}
architecture (Parallel Council + Calibrated Judge) successfully disentangled
conspiracy \textit{topics} from conspiratorial \textit{stance}, overcoming the
Reporter Trap that plagued single-agent classifiers. Critically, the Calibrated
Judge achieved \textbf{100\% accuracy on deadlocks}
(Table~\ref{tab:ablation_judge}), demonstrating that AI can resolve its own
ambiguity when provided with structured debate transcripts rather than mere
vote counts. Our \textbf{Contrastive few-shot retrieval} strategy, hard
negative mining combined with stratified sampling, reduced the False Positive
Rate by 50\% (from 0.160 to 0.080), validating that retrieval strategy design
is as critical as retrieval presence itself.

While the system improves both extraction fidelity and stance discrimination,
we find that pragmatic phenomena such as high-context irony remain challenging
without external user/discourse context (Appendix~\ref{app:qualitative-examples}).

\section{Exploratory Data Analysis}
\label{sec:eda}

\paragraph{Annotation Coverage}
As mentioned in the official website of the task, there are more than 4,100
unique Reddit comments, including 4,800 annotations in total. Most comments
($\sim$3,500), have only one annotation, 550 have two, and 50 have more.
Regarding marker density, around 4,000 comments have at least one
psycholinguistic marker annotation. The exact distribution of marker category
coverage in comments is demonstrated in Figure \ref{fig:num-of-markers}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/num-of-markers.png}
	\caption{Number of marker types in the dataset.}
	\label{fig:num-of-markers}
\end{figure}

\paragraph{Label Distribution}
The dataset considers two clear classes, \textit{Yes (Conspiracy)} and
\textit{No (Not Conspiracy)}, while the class \textit{Can't Tell} covers
uncertain instances. The distribution of labels in the training data is
illustrated in Figure \ref{fig:label-distr}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/label-distr.png}
	\caption{Label distribution for conspiracy detection.}
	\label{fig:label-distr}
\end{figure}
Each marker category (\textsc{Actor}, \textsc{Action}, \textsc{Effect}, \textsc{Evidence}, \textsc{Victim}) appears with different frequency within the dataset. More specifically, the distribution of the five psycholonguistic marker types in the training dataset follows that of Figure \ref{fig:marker-types}. Based on this Figure, we can conclude that conspiracy narratives rely on a small set of recurring rhetorical functions instantiated as markers, but no single function dominates the discourse.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/marker-types.png}
	\caption{Frequency per marker type.}
	\label{fig:marker-types}
\end{figure}

\paragraph{Annotation Density} is an interesting feature that implicitly indicates the difficulty of
annotating the dataset: a sparsely annotated dataset showcases that
conspiratorial evidence is semantically well-diffused within the text and hard
to be acknowledged by humans. Indeed, several documents contain 0 annotations,
while most documents do not exceed 20 annotations. The long-tailed distribution
of markers per document presented in Figure \ref{fig:density} validates the
difficulty of the task.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/annot-density.png}
	\caption{Number of marker annotations per document.}
	\label{fig:density}
\end{figure}

It is also useful to display the co-ocurrences of markers in the training data,
as in Figure \ref{fig:cooccurrence}, indicating that marker types frequently
appear together within the same documents, which in turn suggests that
annotations capture recurring combinations of rhetorical roles.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/cooccurrence.png}
	\caption{Marker type co-occurrences}
	\label{fig:cooccurrence}
\end{figure}
The high self-co-occurrence of \textsc{Action} and \textsc{Actor} markers indicates that many documents describe multiple actions and multiple agents, consistent with narratives that unfold through sequences of events involving several entities rather than isolated claims. The strong co-occurrence between \textsc{Action} and \textsc{Actor} markers further highlights agency attribution as a central organizing principle, with conspiracy narratives frequently linking actors to specific actions. In contrast, \textsc{Effect} and \textsc{Victim} markers show more moderate self-co-occurrence, suggesting that while consequences and affected parties are recurrent elements, they are typically less elaborated than agency and action. Notably, \textsc{Evidence} and \textsc{Victim} markers rarely co-occur within the same documents, indicating a separation between evidential and victim-centered framing. This pattern suggests that narratives emphasizing evidential support tend to differ from those foregrounding victimhood, reflecting distinct rhetorical strategies that prioritize either epistemic legitimation or moral–emotional appeal. Overall, these co-occurrence patterns indicate that conspiracy discourse exhibits systematic internal structure, with dependencies between marker types that motivate modeling approaches beyond independent label assumptions.

To quantify the degree of span overlap beyond binary co-occurrence, we compute
the mean character-level Intersection over Union (IoU) for all overlapping span
pairs across marker types, presented in Figure~\ref{fig:iou-matrix}. The
highest pairwise overlap occurs between \textsc{Actor} and \textsc{Victim}
(mean IoU$=$0.65), reflecting the frequent rhetorical pattern where the accused
party is simultaneously framed as the antagonist and the affected entity.
\textsc{Action}$\leftrightarrow$\textsc{Effect} overlaps are also substantial
(mean IoU$=$0.56), confirming that annotators sometimes struggle to delineate
where a described process ends and its consequence begins. These overlap
patterns directly motivate the S1 Critic's boundary enforcement rules.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/mean-iou-matrix.png}
	\caption{Mean IoU of overlapping spans across marker type pairs. Higher
		values indicate greater boundary ambiguity between categories.}
	\label{fig:iou-matrix}
\end{figure}

\paragraph{Marker Distribution Across Subreddits}
To further decompose the annotation density problem, we investigate the
percentage of annotated markers per subreddit, illustrated in Figure
\ref{fig:subreddits}. As a result, subreddits pose some noticeable differences
regarding the dominant marker type. For example, \textsc{Action} appears rather
stable across subreddits, consistently describing \textit{what is being done},
regardless of community; this demonstrates their foundational nature in
conspiratorial discourse. The role of \textsc{Actor} becomes more prominent in
some communities (Israel\_Palestine) over other rhetorical roles (e.g.
\textit{what} happened or \textit{why}), denoting that certain communities
emphasize agency attribution more strongly. Across all subreddit categories,
\textsc{Actor} constitutes the most dominant marker type. On the contrary,
\textsc{Effect} is one of the less dominant marker types. It appears slightly
lower in (Israel\_Palestine), but slightly elevated in other subreddit
categories, suggesting focus on consequences and outcomes, rather than intent
or causality. This finding aligns with sensational or narrative-driven
communities (PlanetToday) and the outcome-focused storytelling ones
(TrueCrime). \textsc{Evidence} presents some mild variability, becoming less
prominent in Israel\_Palestine and PlanetToday. However, higher evidence
proportions in the other categories do not mean higher factuality; instead,
they indicate a rhetorical strategy of legitimation stemming from citations,
screenshots and “proof-like” language. Finally, \textsc{Victim}, associated
with moralization, emotional appeal and grievance narratives, presents some
noticeable variability, covering higher proportion of markers in PlanetToday
and TrueCrime subreddits.

\begin{figure}[t!]
	\centering
	\includegraphics[width=\linewidth]{images/pct-subreddit.png}
	\caption{Marker type distribution across Subreddits.}
	\label{fig:subreddits}
\end{figure}

\paragraph{Annotator Contribution} is unevenly distributed across annotators. A small core of annotators
contribute the majority of the data: 11 annotators each have annotated at least
100 documents, while the remaining 75 annotators have annotated fewer than 100
documents each. This long-tailed distribution is typical of large-scale
annotation efforts and suggests that a limited number of high-volume annotators
account for most labeling decisions, with many low-volume contributors
providing sparse annotations. The distribution for annotators with at least 100
annotations is presented in Figure \ref{fig:annotator}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/annotator.png}
	\caption{Annotation contribution.}
	\label{fig:annotator}
\end{figure}

\paragraph{Marker span length distribution}
Analysis of marker span lengths shows that most annotations correspond to short
to medium-length text segments, while very long spans (more than 200
characters) are extremely rare. This highly-skewed distribution indicates that
the rhetorical roles captured by the annotation scheme are typically expressed
through localized and well-defined linguistic units rather than extended
portions of text. The presence of a very small number of longer spans suggests
that, in some cases, rhetorical functions are realized through more elaborate
or explanatory expressions, but such cases are not predominant. Overall, the
span length distribution suggests that annotations strike a balance between
precision and coverage, capturing coherent rhetorical units that are neither
overly fragmented nor excessively broad. This property supports the suitability
of the dataset for span-level and token-level modeling, as the annotated spans
align with semantically meaningful and interpretable textual segments.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/span-length.png}
	\caption{Span length distribution.}
	\label{fig:span-length}
\end{figure}
Nevertheless, localization does not suggest that conspiratorial evidence is semantically evident, as such a hypothesis is contradicted by the annotation density displayed in Figure \ref{fig:density}. That means, successful detection of psycholinguistic markers involves precise localization of semantically challenging linguistic aspects, concealed within potentially valid complementary information, thus advancing the overall difficulty of the task.

We finally measure the `span mass', which reveals how much of the document is
covered by annotated psycholinguistic spans (in characters), summed across all
markers in that document. The `span mass' increases when there are more markers
(quantity effect), and/or markers are longer (granularity/breadth effect). The
trend is illustrated in Figure \ref{fig:mass}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{images/span-mass.png}
	\caption{Marker span mass.}
	\label{fig:mass}
\end{figure}

The relationship between total marker span length and the number of markers per
document exhibits a clear positive trend, indicating that annotation coverage
scales approximately linearly with annotation density. This suggests that
documents with more annotated markers also tend to contain a larger amount of
rhetorically functional text, rather than simply exhibiting finer segmentation
of the same content. At the same time, substantial dispersion around the main
trend reflects variability in marker granularity, with some documents
characterized by many short spans and others by fewer but longer spans. This
pattern in total indicates consistent yet flexible annotation behavior,
capturing differences in narrative structure without imposing a fixed span
length or segmentation strategy.

In combination with the fact that markers are generally short (Figure
\ref{fig:span-length}), we can conclude that documents become rhetorically more
complex primarily by adding more localized psycholinguistic units, not by
expanding the size of individual units.

\paragraph{Span Position Analysis.}
Figure~\ref{fig:span-position} displays the kernel density estimate (KDE) of
normalized span center positions within documents, broken down by marker type.
\textsc{Actor} spans concentrate toward the beginning of documents (median
position$=$0.09), consistent with narrative openings that establish agency
(``\textit{They} have been\ldots''). In contrast, \textsc{Effect} spans peak
later (median position$=$0.43), reflecting their role as narrative consequences
that follow causal chains. \textsc{Evidence} spans exhibit the broadest
positional spread, appearing throughout documents as authors interleave claims
with supporting citations. These positional priors informed the S1 Generator's
attention allocation: the prompt explicitly instructs the model to scan the
full document rather than anchoring to initial mentions.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/span-position-analysis.png}
	\caption{Normalized position of marker spans within documents
		(0$=$start, 1$=$end). KDE per marker type.}
	\label{fig:span-position}
\end{figure}

\paragraph{EDA-Driven Design Decisions.}
Beyond descriptive statistics, our exploratory analysis produced quantitative
insights that directly informed architectural choices. Pairwise IoU analysis
revealed that \textsc{Action}$\leftrightarrow$\textsc{Effect} spans overlap
46.4\% of the time at IoU$\geq$0.5 (mean IoU$=$0.56, 95\% CI [0.52,\,0.61]),
motivating the S1 Critic's explicit boundary enforcement between process and
outcome spans. Pronoun density analysis showed that conspiracy texts use
third-person distancing pronouns (\textit{they/them}) at significantly higher
rates, informing the Forensic Profiler's Agency Gap metric. Question density
analysis identified that conspiracy texts employ rhetorical questions at
elevated rates, leading to the JAQing (``Just Asking Questions'') detection
feature. Mann--Whitney tests with Benjamini--Hochberg correction confirmed that
absolutist language rates differ significantly between conspiracy and
non-conspiracy documents ($p_{\text{adj}}<0.001$, Cliff's $\delta=0.05$),
validating the inclusion of epistemic intensity as a forensic profiler feature.
Hard example mining via a TF-IDF baseline classifier identified documents where
confident predictions were incorrect, directly informing the hard negative
selection strategy for contrastive retrieval. These findings are reproducible
from the \texttt{analysis-and-insights.py} script, with all derived artifacts
archived in the supplementary material.

\section{Prompts}
\label{sec:prompts}

All system and user prompts in our pipeline are formatted using
\textbf{XML-structured markup}, where hierarchical tags delineate prompt
sections (role definitions, extraction ontologies, output schemas, execution
protocols). This design choice is motivated by three converging lines of
evidence:

\paragraph{Structured Boundary Enforcement.}
LLM-integrated applications are vulnerable to \textit{indirect prompt
	injection}, where the boundary between instructions and data is blurred
\cite{greshake2023indirect}. In our pipeline, user-submitted Reddit text is
injected into prompts alongside complex multi-section instructions. XML tags
(e.g., \texttt{\textless source\_document\textgreater}, \texttt{\textless
	extraction\_ontology\textgreater}, \texttt{\textless
	output\_format\textgreater}) create unambiguous structural delimiters that
prevent the model from confusing document content with system directives, a
critical concern when processing adversarial or conspiratorial text that may
contain imperative language.

\paragraph{Hierarchical Parsing.}
Recent work formalizes XML prompting as grammar-constrained interaction,
demonstrating that tree-structured prompts enable LLMs to parse complex
multi-part instructions more reliably than flat text
\cite{alpay2025xmlprompting, sambaraju2025xmlstructured}. Our prompts nest up
to three levels deep (e.g., \texttt{\textless system\_directive\textgreater}
then \texttt{\textless extraction\_ontology\textgreater} then \texttt{\textless
	category\textgreater}), mirroring the compositional structure of the task
itself. Both Anthropic \cite{anthropic2024xml} and OpenAI
\cite{openai2024prompting} explicitly recommend XML tags for structuring
complex prompts, noting improved accuracy and reduced misinterpretation. This
approach follows the broader trend of treating prompts as structured programs
rather than ad-hoc text strings \cite{white2023prompt}.

\paragraph{Notation.} In the listings below, \texttt{\{\{variable\}\}} denotes runtime-injected
values (document text, few-shot retrieval context, forensic statistics). All
prompts were optimized via GEPA (Appendix~\ref{sec:gepa-details}); we present
the final evolved versions.

\subsection{S1: DD-CoT Generator}
\label{sec:prompt-generator}

The generator prompt establishes the ``Conspiracy-Marker Extractor'' persona
with a five-step pipeline: (1)~a \textbf{Neutrality Gate} that filters negative
examples before extraction, (2)~an \textbf{Assertion vs.\ Discussion} check
distinguishing endorsed claims from reported ones, (3)~\textbf{Dominant
	Narrative} classification, (4)~the \textbf{Triangle of Malice} extraction
ontology (\textsc{Actor}, \textsc{Action}, \textsc{Effect}, \textsc{Victim}, \textsc{Evidence}) with positive and negative
examples for each category, and (5)~\textbf{Span Rules} enforcing verbatim
extraction and hallucination prevention.

\lstinputlisting[caption={S1 DD-CoT Generator (System Prompt)},label=lst:s1-gen]{prompts/openai/s1_ddcot_generator_optimized.txt}

\lstinputlisting[caption={S1 DD-CoT Generator (User Prompt)},label=lst:s1-gen-user]{prompts/openai/s1_ddcot_user_optimized.txt}

\subsection{S1: Forensic QA Critic}
\label{sec:prompt-critic}

The Critic audits generator output through a five-check pipeline. Its most
critical innovation is the \textbf{Negative-Example Gating} check
(Appendix~\ref{sec:gepa-details}): before auditing span quality, the Critic
first verifies whether the source text contains \emph{any} conspiracy markers
at all, ordering wholesale span deletion for negative examples. Subsequent
checks address \textbf{Frame Leakage} (attribution prefixes bleeding into
spans), \textbf{Span Bloat} (action spans exceeding verb + direct object),
\textbf{Reporter Trap} label accuracy, and \textbf{Lazy Verb} detection
(existential verbs without mechanistic content).

\lstinputlisting[caption={S1 Forensic QA Critic (System Prompt)},label=lst:s1-critic]{prompts/openai/s1_ddcot_critic_optimized.txt}

\lstinputlisting[caption={S1 Critic (User Prompt)},label=lst:s1-critic-user]{prompts/openai/s1_ddcot_critic_user_optimized.txt}

\subsection{S1: DD-CoT Refiner}
\label{sec:prompt-refiner}

The Refiner executes Critic change orders with surgical precision through five
protocols: \textbf{Trim} (bloat fix), \textbf{Strip Frames} (remove attribution
prefixes), \textbf{Label Correction}, \textbf{Add Missed Spans}, and
\textbf{Prune Hallucinations}. A critical design constraint is the
\textbf{Decision Rule}: the Refiner may only add new spans if the Critic
explicitly listed them in \texttt{missed\_spans}, preventing hallucinated
insertions in negative examples.

\lstinputlisting[caption={S1 DD-CoT Refiner (System Prompt)},label=lst:s1-refiner]{prompts/openai/s1_ddcot_refiner_optimized.txt}

\lstinputlisting[caption={S1 Refiner (User Prompt)},label=lst:s1-refiner-user]{prompts/openai/s1_ddcot_refiner_user_optimized.txt}

\subsection{S2: Council Juror Prompts}
\label{sec:prompt-council}

Each juror receives a shared \textbf{case file} (Listing~\ref{lst:s2-user})
containing the source text, subreddit context, forensic signals, S1 marker
summary, retrieval-selected few-shot legal precedents, and voting instructions.
The case file implements a context-aware \textbf{Standard of Proof}: subreddits
like \texttt{r/conspiracy} trigger a presumption of guilt, while mainstream
sources like \texttt{r/news} trigger a presumption of innocence. Each juror
then processes this evidence through their persona-specific system prompt.

\lstinputlisting[caption={S2 Council Case File (shared user prompt)},label=lst:s2-user]{prompts/openai/s2_parallel_user_optimized.txt}

The four juror system prompts encode complementary adjudication perspectives.
All share a common \textbf{Structural Assertion Rule}: statements asserting the
existence of a conspiracy as fact (e.g., ``There has been a conspiracy to
undermine\ldots'') constitute \textbf{Endorsement by Assertion}, regardless of
passive voice or formal tone. Each also receives retrieved \textbf{few-shot
	precedents} from similar past cases via the \texttt{\{\{rag\_context\}\}}
variable.

\lstinputlisting[caption={S2 Prosecutor (System Prompt)},label=lst:s2-prosecutor]{prompts/openai/s2_parallel_prosecutor_optimized.txt}

\lstinputlisting[caption={S2 Defense Attorney (System Prompt)},label=lst:s2-defense]{prompts/openai/s2_parallel_defense_optimized.txt}

\lstinputlisting[caption={S2 Literalist (System Prompt)},label=lst:s2-literalist]{prompts/openai/s2_parallel_literalist_optimized.txt}

\lstinputlisting[caption={S2 Forensic Profiler (System Prompt)},label=lst:s2-profiler]{prompts/openai/s2_parallel_profiler_optimized.txt}

\subsection{S2: Calibrated Judge}
\label{sec:prompt-judge}

The Judge prompt implements calibrated adjudication as the final arbiter. Key
elements include: a \textbf{Standard of Proof} based on structural endorsement
of malice (not mere discussion), a \textbf{Forensic Priors Checklist} for
interpreting quantitative signals (\texttt{uncertainty\_ratio},
\texttt{epistemic\_intensity}, \texttt{agency\_gap}, \texttt{is\_jaqing}),
\textbf{Council Synthesis Rules} requiring rationale-level analysis rather than
vote counting, and an \textbf{Appeal Override} mechanism for mandatory
adversarial review cases.

\lstinputlisting[caption={S2 Calibrated Judge (System Prompt)},label=lst:s2-judge]{prompts/openai/s2_calibrated_judge_optimized.txt}

\lstinputlisting[caption={S2 Judge (User Prompt: Case File)},label=lst:s2-judge-user]{prompts/openai/s2_calibrated_judge_user_optimized.txt}

\section{GEPA Implementation Details}
\label{sec:gepa-details}

This appendix provides detailed implementation specifics of the Genetic
Evolution Prompt Algorithm (GEPA) \cite{agrawal2025gepa} as integrated with
MLflow \cite{mlflow2024} for automated prompt optimization in our
psycholinguistic conspiracy marker detection system.

\subsection{Overview}

GEPA is an evolutionary meta-optimization framework that treats prompt
engineering as a search problem over the space of natural language
instructions. Unlike traditional genetic algorithms that operate on
fixed-length binary strings, GEPA evolves \textit{natural language prompts}
through a combination of tournament selection, LLM-guided crossover, and
reflective mutation. The framework is integrated into the MLflow ecosystem via
the \texttt{mlflow.genai.optimize\_prompts} API and the
\texttt{GepaPromptOptimizer} configuration class.

\subsection{Population Management}

\paragraph{Initialization.} The evolutionary process begins with a seed population of $N = 20$--$30$ prompt
variants registered as versioned artifacts in MLflow's prompt registry. Each
candidate is stored with a unique URI (e.g.,
\texttt{models:/s1\_ddcot\_generator/v3}) enabling reproducibility and
rollback. The initial population typically consists of:

\begin{itemize}[noitemsep]
	\item \textbf{Manual baseline}: Hand-crafted prompts from domain experts
	\item \textbf{Synthetic perturbations}: Rule-based variations (e.g., reordering instruction clauses, paraphrasing definitions)
	\item \textbf{Historical best}: Top performers from prior optimization runs
\end{itemize}

\paragraph{Generational evolution.} The population evolves over $G = 40$--$80$ generations (controlled by the
\texttt{max\_metric\_calls} parameter), with each generation consisting of:
\begin{enumerate}[noitemsep]
	\item Fitness evaluation of all candidates on evaluation set
	\item Selection of top-$k$ parents ($k = \lceil 0.3N \rceil$)
	\item Crossover and mutation to generate $N - k$ offspring
	\item Replacement of bottom performers with offspring
\end{enumerate}

\paragraph{Convergence criteria.} Optimization terminates when either: (a) the budget is exhausted, or (b) the
population converges, defined as $\max F_i - \min F_i < \epsilon$ where $F_i$
is the fitness of candidate $i$ and $\epsilon = 0.02$ (2\% improvement
threshold).

\subsection{Selection Mechanism}

GEPA employs \textbf{tournament selection} to choose parent prompts for
breeding:

\begin{enumerate}[noitemsep]
	\item Randomly sample $k_{\text{tour}} = 3$ candidates from the population
	\item Evaluate fitness $F_i$ for each candidate using the custom scorer (described in
	      \S\ref{sec:gepa-fitness})
	\item Select the candidate with highest $F_i$ as parent
	\item Repeat to obtain a second parent (sampling without replacement to ensure
	      diversity)
\end{enumerate}

This stochastic selection mechanism balances \textbf{exploitation} (favoring
high-fitness prompts) with \textbf{exploration} (giving lower-fitness
candidates a non-zero probability of selection). Compared to deterministic
top-$k$ selection, tournament selection reduces premature convergence to local
optima in the high-dimensional prompt space.

\paragraph{Selection pressure.} The tournament size $k_{\text{tour}}$ controls selection pressure: smaller
values increase diversity (risk of random drift), while larger values intensify
competition (risk of premature convergence). We empirically set
$k_{\text{tour}} = 3$ based on pilot experiments comparing convergence speed
vs.\ final performance.

\subsection{Crossover Operation}

Unlike classical genetic algorithms that perform single-point or uniform
crossover on bit strings, GEPA uses an \textbf{LLM-guided semantic crossover}
to merge two parent prompts:

\begin{lstlisting}[language=Python, caption=Crossover Prompt Template (Pseudocode)]
CROSSOVER_PROMPT = """
You are optimizing prompts for a conspiracy marker extraction task.

**Parent Prompt A** (F1 = {fitness_a}):
{prompt_a}

**Parent Prompt B** (F1 = {fitness_b}):
{prompt_b}

Create a NEW prompt that COMBINES the strengths of both parents:
1. Identify which instructions/constraints are effective in each parent
2. Merge complementary elements (avoid redundant repetition)
3. Remove contradictory or low-value instructions
4. Ensure the offspring prompt is coherent and actionable

**Constraints**:
- Preserve ALL variable placeholders (e.g., {{few_shot_examples}})
- Do not exceed the token budget of the larger parent
- Maintain the same output schema

Return ONLY the new prompt text (no explanation).
"""
\end{lstlisting}

The crossover model (typically GPT-5.2 or Claude Sonnet) performs a
\textbf{semantic diff-merge}: it extracts high-level strategic elements (e.g.,
``Check for attribution verbs before labeling as Actor'') rather than
performing character-level splicing. This approach respects the discrete,
compositional structure of natural language prompts, where naive substring
concatenation would produce incoherent outputs.

\paragraph{Fitness-weighted crossover.} To bias offspring toward higher-performing lineages, we provide fitness scores
$F_A$ and $F_B$ to the crossover model. Empirically, we observe that LLMs
implicitly weight instructions from the higher-fitness parent more heavily,
though this behavior is not explicitly enforced in the prompt.

\subsection{Mutation Details}

GEPA's key innovation is \textbf{reflective LLM mutation}, which replaces
random perturbation with targeted, feedback-driven edits:

\paragraph{Mutation trigger.} Mutation is applied to:
\begin{itemize}[noitemsep]
	\item All newly generated offspring (post-crossover)
	\item Randomly selected individuals from the surviving parent population (mutation
	      rate $p_m = 0.2$)
\end{itemize}

\paragraph{Mutation prompt.} The reflector LLM (GPT-5.2 in our experiments) receives:

\begin{lstlisting}[language=Python, caption=Mutation Prompt Template (Pseudocode)]
MUTATION_PROMPT = """
You are a prompt optimization expert analyzing failure patterns.

**Current Prompt** (F1 = {current_fitness}):
{current_prompt}

**Recent Errors** (from scorer feedback):
{aggregated_feedback}

Examples:
- "FIX LABELS: 'NASA' should be Actor not Evidence"
- "EXTRACT MISSING: [Effect] 'public distrust'"
- "HALLUCINATED: Remove 'the government' (not in text)"

**Task**: Propose a SINGLE targeted edit to improve this prompt:
1. Analyze the error patterns to identify root cause
2. Suggest ONE concrete instruction change (add/remove/revise)
3. Justify why this edit addresses the failure mode

**Constraints**:
- Make MINIMAL changes (one instruction at a time)
- Preserve variable placeholders
- Do not contradict existing high-performing constraints

Return: {"edit": "<your edit>", "rationale": "<why this helps>"}
"""
\end{lstlisting}

\paragraph{Rich feedback integration.} The \texttt{\{aggregated\_feedback\}} field aggregates scorer rationale from
the past $B = 5$ evaluation rounds, prioritizing:
\begin{enumerate}[noitemsep]
	\item \textbf{Critical errors} (label misclassifications)
	\item \textbf{Recall gaps} (missed spans)
	\item \textbf{Precision noise} (hallucinated spans)
	\item \textbf{Boundary errors} (IoU $< 0.7$)
\end{enumerate}

This structured feedback enables the reflector to diagnose \textit{why}
predictions fail rather than merely observing \textit{that} they fail. For
example:

\begin{itemize}[noitemsep]
	\item \textbf{Pattern}: Model misclassifies reporting-style text (``The article claims X'') as conspiracy endorsement
	\item \textbf{Mutation}: Add instruction: ``Check for attribution verbs (claims, alleges, reports) indicating neutral summarization.''
\end{itemize}

\paragraph{Mutation acceptance.} Mutated prompts are re-evaluated, and the mutation is accepted only if
$F_{\text{mutant}} > F_{\text{parent}} - \delta$, where $\delta = 0.01$ is a
tolerance threshold allowing slight fitness decreases to escape local optima.
Rejected mutations are discarded, and the parent continues to the next
generation unmodified.

\subsection{Fitness Evaluation}
\label{sec:gepa-fitness}

GEPA evaluates prompt fitness using task-specific custom scorers that return
both \textbf{numeric metrics} and \textbf{textual rationale}:

\paragraph{S1 Scorer (Span Extraction).} The scorer computes:
\begin{align*}
	\text{Precision} & = \frac{\text{\# correct predictions}}{\text{\# predicted spans}}            \\
	\text{Recall}    & = \frac{\text{\# correct predictions}}{\text{\# gold spans}}                 \\
	F_\beta          & = \frac{(1 + \beta^2) \cdot P \cdot R}{\beta^2 \cdot P + R}, \quad \beta = 2
\end{align*}

We use $\beta = 2$ to prioritize recall over precision, as the downstream S2
classification task is more robust to false positive marker spans than to
missing true positives.

\paragraph{S1 Actionable Feedback.} The scorer generates structured, numbered feedback:

\begin{enumerate}[noitemsep]
	\item \textbf{SUCCESS (Positive Reinforcement):} ``KEEP DOING: Correctly extracted 4/5 spans (e.g., `Big Pharma', `suppressed cures')'' (locks in successful behaviors).
	\item \textbf{CRITICAL (Logic Errors):} ``FIX LABELS: `NASA' should be \textsc{Actor} not \textsc{Evidence}; `released fake photos' should be \textsc{Action} not \textsc{Effect}''
	\item \textbf{REFINEMENT (Boundary Issues):} ``TIGHTEN BOUNDARIES: `approved' should be `approved the expensive prices'\,''
	\item \textbf{RECALL (Missing Spans):} ``EXTRACT MISSING: \textsc{Actor} `the government' at position 45--55''
	\item \textbf{NOISE (Hallucinations):} ``REMOVE: Hallucinated span `everyone knows' not in source text''
\end{enumerate}

This hierarchical structure enables the reflector to prioritize high-impact
edits (label errors $>$ boundary errors $>$ noise reduction).

\paragraph{S2 Scorer (Classification).} Rather than using binary accuracy, the S2 scorer implements a \textbf{gradient
	consensus} metric based on council vote ratios, rewarding partial correctness
even when the final aggregated verdict is wrong. The scorer computes: $$
	F_{\text{gradient}} = \begin{cases}
		1.0                                         & \text{if } \hat{y} = y    \\
		\frac{N_{\text{correct}}}{N_{\text{total}}} & \text{if } \hat{y} \neq y
	\end{cases}
$$
where $N_{\text{correct}}$ is the number of jurors that voted for the correct
label and $N_{\text{total}}$ is the total number of votes cast. When the gold
label is ``conspiracy,'' the score equals the proportion of conspiracy votes;
when ``non,'' the proportion of non-conspiracy votes. This gradient signal
encourages the optimizer to improve per-juror reasoning even when the final
aggregated verdict is incorrect, providing a smoother fitness landscape than
binary accuracy. The scorer additionally generates structured actionable
feedback with prioritized error categories: positive anchoring for correct
verdicts, calibration warnings for overconfident errors ($>$0.85), hard
negative trap identification, dissent analysis (highlighting jurors who voted
correctly when the majority was wrong), and judge override detection.

\subsection{The ``Trojan Horse'' Pattern}

\paragraph{Problem.} MLflow's \texttt{optimize\_prompts} API sanitizes target data in the
\texttt{outputs} field to prevent label leakage during evaluation. However,
custom scorers require access to gold labels to compute metrics and generate
feedback.

\paragraph{Solution.} We inject gold labels into the \texttt{inputs} dictionary, creating a
\textbf{passthrough tunnel} through the prediction wrapper:

\begin{lstlisting}[language=Python, caption=Trojan Horse Implementation]
# In load_eval_data():
dataset.append({
    "inputs": {
        "text": row["text"],
        "passthrough_gold": json.dumps({
            "gold_spans": gold_spans,      # S1 labels
            "gold_label": gold_label,      # S2 labels
            "doc_id": doc_id,
        }),
    },
    "outputs": {"dummy_target": "ignore_me"},
})

# In predict_wrapper():
def predict_wrapper(text, passthrough_gold, ...):
    # Run inference (ignores passthrough_gold)
    predictions = model.predict(text)
    
    # Echo passthrough_gold to outputs for scorer access
    return {
        "predictions": predictions,
        "passthrough_gold_ref": passthrough_gold,  # Tunnel
    }

# In custom scorer:
@scorer
def s1_rich_scorer(outputs, expectations):
    gold_data = json.loads(outputs["passthrough_gold_ref"])
    gold_spans = gold_data["gold_spans"]
    pred_spans = outputs["predictions"]
    # Compute metrics and feedback...
\end{lstlisting}

This pattern preserves MLflow's sanitization logic while enabling rich
diagnostic feedback. The \texttt{passthrough\_gold} field is ignored during
inference (it does not influence model predictions), but is accessible to the
scorer for evaluation.

\subsection{Hyperparameter Configuration}

Table~\ref{tab:gepa-config} summarizes the hyperparameters used in our
optimization runs:

\begin{table}[h!]
	\centering
	\small
	\resizebox{\linewidth}{!}{
	\begin{tabular}{@{}lll@{}}
		\toprule
		\textbf{Parameter}         & \textbf{Value}     & \textbf{Description}                          \\
		\midrule
		Population size            & 20--30             & Number of prompt candidates                   \\
		Max generations            & 40--80             & Budget (max\_metric\_calls)                   \\
		Tournament size            & 3                  & Selection pressure                            \\
		Mutation rate              & 0.2                & Probability of mutating survivors             \\
		Crossover model            & GPT-5.2            & LLM for semantic merging                      \\
		Reflector model            & GPT-5.2            & LLM for mutation feedback                     \\
		Convergence threshold      & 0.02               & Fitness plateau tolerance                     \\
		Mutation acceptance margin & 0.01               & Fitness drop tolerance ($\delta$)             \\
		Feedback history window    & 5 generations      & Error aggregation depth                       \\
		S1 fitness metric          & $F_2$ (Macro)      & Recall-biased F-score ($\beta = 2$)           \\
		S2 fitness metric          & Gradient Consensus & Vote-ratio scoring (\S\ref{sec:gepa-fitness}) \\
		\bottomrule
	\end{tabular}
	}
	\caption{GEPA hyperparameter configuration for S1 and S2 optimization.}
	\label{tab:gepa-config}
\end{table}

\subsection{Performance Gains}

Automated prompt optimization via GEPA yielded significant absolute
improvements over hand-crafted baselines:

\begin{itemize}[noitemsep]
	\item \textbf{S1 (DD-CoT)}: $F_1$ improved from 0.72 to 0.81 (+12.5\%)
	\item \textbf{S2 (Anti-Echo Council)}: Accuracy improved from 0.78 to 0.84 (+7.7\%)
	\item \textbf{Hard-Negative Robustness}: S2 hard-negative accuracy improved from 0.62 to 0.79 (+27.4\%)
\end{itemize}

\paragraph{Convergence analysis.} Fitness typically plateaus after 50--60 evaluations, with 90\% of final
improvement achieved within the first 30 generations. This suggests that GEPA
efficiently exploits the prompt space without requiring exhaustive search.

\paragraph{Failure mode analysis.} Rejected mutations most commonly attempt to:
\begin{enumerate}[noitemsep]
	\item Add redundant constraints that conflict with existing instructions
	\item Over-specify edge cases, harming generalization
	\item Introduce verbose explanations that exceed token budgets
\end{enumerate}

These failure modes validate the importance of \textbf{minimal edits} and
\textbf{structured feedback} in guiding effective mutations.

\section{Portability to Open-Weights Models}
\label{app:open-weights}
We additionally evaluated a smaller, open-weights model, Qwen-3-8B-Instruct \cite{qwen3}, to test the architectural portability of our approach. Due to the model's limited tool-calling fidelity---a known challenge for smaller models attempting complex API schemas \cite{patil2023gorilla}---and reduced adherence to multi-turn instructions, the full DD-CoT schema proved too complex. Instead, we deployed a \textbf{Lite S1 Agent} using simplified Pydantic schemas (3 fields vs.\ 10) and no self-refinement loop. Similarly, for S2, we simplified the Council to a \textbf{Dyadic Debate} (Prosecutor vs.\ Defense) followed by a Judge, removing the Literalist and Profiler roles to reduce context load. Despite these simplifications, Qwen-3-8B achieved 0.16 macro F1 on S1 and 0.63 weighted F1 on S2 (Dev set). While lower than the full GPT-5.2 system (0.24 / 0.79), these results remain competitive with the organizer baselines (approx 0.15 / 0.76), suggesting that the core agentic reasoning transfers meaningfully even to 8B-scale models with reduced schema complexity.

\section{Detailed Qualitative Analysis and Error Patterns}
\label{app:qualitative-examples}
To better understand the mechanism of improvement, we analyze specific linguistic phenomena where the agentic workflow succeeds or fails compared to the baseline.

\paragraph{Success: Disentangling Agency via Discrimination.}
A major source of S1 error is the confusion between grammatical subjects and
semantic agents, particularly in passive constructions. For example, in the
sentence \textit{``The public was manipulated by the media to distrust
	vaccines,''} standard CoT often tags \textit{``The public''} as \textsc{Actor}
due to its subject position. The DD-CoT Generator, forced to provide a
counter-argument (e.g., ``Why is `The public' NOT an Actor?''), correctly
identifies it as a \textsc{Victim} and attributes agency to \textit{``the
	media.''} This discriminative step drives the +2.7 point gain in \textsc{Actor}
F1, demonstrating that agency detection requires explicit reasoning about
semantic roles rather than surface syntax.

\paragraph{Success: Mitigating the Reporter Trap.}
The ``Reporter Trap''---misclassifying neutral reporting of conspiracies as
endorsement---is the dominant failure mode for zero-shot models. The baseline
frequently flags phrases like \textit{``The article claims that...''} as
evidence of conspiracy. Our approach mitigates this through two mechanisms: (i)
\textbf{Contrastive Retrieval} injects hard negatives (texts containing marker
vocabulary but opposing stance) into the context, and (ii) the \textbf{Defense
	Attorney} agent specifically parses attribution verbs (\textit{said, claimed,
	reported}). This combination effectively teaches the model to distinguish
between the \textit{mention} of a conspiracy and the \textit{act} of
conspiring.

\paragraph{Limitation: High-Context Irony and Poe's Law.}
Persistent errors cluster around implicit stance, particularly sarcasm and
``Poe's Law'' scenarios where extreme views are parodied without explicit
markers. For instance, Reddit comments that mimic conspiratorial style to mock
it (\textit{``Oh sure, and the earth is flat too!''}) are occasionally flagged
as endorsement by the Literalist agent, while the Profiler captures the
sarcasm. When these signals conflict, the conservative Judge tends to default
to \texttt{non-conspiracy}, occasionally reducing recall. This suggests that
detecting high-context irony requires broader discourse-level features (e.g.,
user history or thread structure) beyond the scope of a single-turn document
analyzer.

\section{Detailed Council and Judge Specification}
\label{app:council-details}

\paragraph{Juror Output Schema.} Each juror in the Parallel Council produces a structured JSON output with the
following fields: (i) a binary \texttt{verdict} (\texttt{conspiracy} or
\texttt{non}); (ii) a scalar \texttt{confidence} $c \in [0,1]$; (iii) a
\texttt{key\_signal} containing the verbatim textual evidence supporting the
verdict; (iv) a mandatory \texttt{steelman\_opposing} argument responding to
the strongest counter-perspective; and (v) \texttt{uncertainty\_flags} for
specific ambiguities (e.g., \textsc{Reporting}, \textsc{Sarcasm}, \textsc{Poe's
	Law}).

\paragraph{Calibrated Judge Weighting.} The Judge computes a weighted consensus score:
\[ W = \sum_{j=1}^{4} \begin{cases} +c_j & \text{if } v_j = \texttt{conspiracy} \\ -c_j & \text{if } v_j = \texttt{non} \end{cases} \]
where $c_j$ is juror confidence and $v_j$ is the verdict. We apply conservative
confidence thresholds: for split councils ($2$--$2$), final confidence is
capped at $0.75$, and the case is marked \texttt{borderline}, defaulting to
\texttt{non} if evidence remains ambiguous. Overrides (Judge voting against a
3--1 majority) are triggered only by critical forensic signals (e.g., high
\texttt{uncertainty\_ratio}).

\paragraph{Baseline Comparability and Reproducibility Meta-Discussion.}
\label{app:exp-meta}
The organizer's starter-pack baselines (approx 0.15 overlap $F_1$ and 0.76 weighted $F_1$) utilize different metric definitions than our macro-$F_1$ reporting; thus, $\Delta$ improvements are reported relative to our internal zero-shot GPT-5.2 baseline to ensure a controlled comparison. Regarding reproducibility, floating-point non-associativity in Mixture-of-Experts (MoE) architectures \cite{thinkingmachines_nondeterminism} introduces minor variance in extraction boundaries, which we mitigate via multi-run validation and hierarchical auditing nodes ($\tau=0.0$).

\end{document}
