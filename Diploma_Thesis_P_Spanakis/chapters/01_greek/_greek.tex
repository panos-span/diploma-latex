\chapter*{Εκτεταμένη Περίληψη στα Ελληνικά}
\addcontentsline{toc}{chapter}{Εκτεταμένη Περίληψη στα Ελληνικά}
\setcounter{figure}{0}
\renewcommand{\thefigure}{\arabic{figure}}

\section*{Εισαγωγή}

Οι άνθρωποι εδώ και καιρό εμφανίζουν τάση να υιοθετούν θεωρίες συνωμοσίας, ιδιαίτερα σε πλαίσια αβεβαιότητας, απειλής και κοινωνικής αναταραχής. Τέτοιες πεποιθήσεις δημιουργούνται και διαδίδονται για να εξυπηρετήσουν την ανάγκη του ανθρώπου να αντιμετωπίσει υπαρξιακά ή κοινωνικά ζητήματα και να ενισχύσει την αίσθηση ταυτότητας και κοινωνικού ανήκειν. Παρά την ψυχολογική τους ελκυστικότητα, οι θεωρίες συνωμοσίας συνδέονται με επιβλαβείς συνέπειες, περιορίζοντας την εμπιστοσύνη σε τεκμηριωμένα γεγονότα και δημόσιες αποφάσεις, ενώ ταυτόχρονα επιδεινώνουν την πολιτική πόλωση και τα μοτίβα παραπληροφόρησης.

Η άνοδος της τεχνητής νοημοσύνης ενέπνευσε τη σύνδεση της αναγνώρισης συνωμοσιών με τη φυσική γλώσσα, δεδομένου ότι η γλώσσα αποτελεί το πλέον θεμελιώδες μέσο για τη διατύπωση και τη διάδοση συνωμοσιών. Τα Μεγάλα Γλωσσικά Μοντέλα (Large Language Models -- LLMs) έχουν φέρει επανάσταση τη γλωσσική έρευνα, επιτρέποντας τη βαθιά αναγνώριση μοτίβων και τη διάκριση μεταξύ αυτών. Ωστόσο, τα Μεγάλα Γλωσσικά Μοντέλα (LLMs) παρουσιάζουν σημαντική ευπάθεια σε γνωστικά σφάλματα και χειραγώγηση μέσω πειστικής γλώσσας, ενώ παράγουν και ενισχύουν παραπληροφόρηση.

Για την αντιμετώπιση αυτών των προκλήσεων, το \textbf{SemEval 2026 Task 10} δίνει έμφαση στον εντοπισμό και την κατηγοριοποίηση γλωσσικών δεικτών που σηματοδοτούν συνωμοσιακή σκέψη, συμπληρώνοντας την ανίχνευση με ψυχολογικά τεκμηριωμένους σχολιασμούς. Στην παρούσα εργασία, αξιοποιούμε Μεγάλα Γλωσσικά Μοντέλα (LLMs) εντός πρακτορικών δομών για την αναγνώριση της συνωμοσιακής σκέψης. Εξ όσων γνωρίζουμε, εισάγουμε την \textit{πρώτη πρακτορική μέθοδο βασισμένη σε Μεγάλα Γλωσσικά Μοντέλα (LLMs)} για την αντιμετώπιση της ανίχνευσης συνωμοσιών και του εντοπισμού ψυχογλωσσικών χαρακτηριστικών στη γλώσσα.

\section*{Θεωρητικό Υπόβαθρο}

\subsection*{Μεγάλα Γλωσσικά Μοντέλα (LLMs)}
 
 Τα Μεγάλα Γλωσσικά Μοντέλα (LLMs) αποτελούν μια κατηγορία συστημάτων τεχνητής νοημοσύνης εκπαιδευμένων να κατανοούν και να παράγουν ανθρώπινη γλώσσα μέσω μάθησης από τεράστια σώματα δεδομένων. Βασίζονται κυρίως στην αρχιτεκτονική Transformer, η οποία αξιοποιεί μηχανισμούς Self-Attention για τον παράλληλο υπολογισμό πλαισιοποιημένων αναπαραστάσεων. Οι scaling laws περιγράφουν πώς η απόδοση βελτιώνεται προβλέψιμα καθώς αυξάνονται το μέγεθος μοντέλου, τα δεδομένα εκπαίδευσης και η υπολογιστική ισχύς. Μία ιδιαίτερα ενδιαφέρουσα πτυχή των Μεγάλων Γλωσσικών Μοντέλων (LLMs) αποτελούν οι αναδυόμενες ικανότητες---δηλαδή ικανότητες που εμφανίζονται ξαφνικά μόλις το μοντέλο υπερβεί ένα ορισμένο κατώφλι κλίμακας, χωρίς σταδιακή βελτίωση πριν από αυτό.

\subsubsection*{Αρχιτεκτονική Transformer σε Λεπτομέρεια}

Η αρχιτεκτονική Transformer εισήχθη από τους Vaswani et al. (2017) και αντικατέστησε τα RNNs ως βασικό δομικό στοιχείο γλωσσικής μοντελοποίησης. Αντί να επεξεργάζεται ένα token τη φορά ακολουθιακά, ο Transformer επεξεργάζεται ολόκληρη την ακολουθία εισόδου παράλληλα μέσω του μηχανισμού αυτο-προσοχής (Self-Attention). Κάθε token μπορεί να «προσέξει» κάθε άλλο token, υπολογίζοντας σταθμισμένο άθροισμα αναπαραστάσεων βάσει συμβατότητας Query-Key:
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
όπου $Q$, $K$, $V$ είναι οι προβολές Query, Key, and Value, και $d_k$ η διάσταση κλειδιού.

Η Πολλαπλή Κεφαλή Προσοχής (Multi-Head Attention) εκτελεί $h$ παράλληλους μηχανισμούς attention με διαφορετικές μαθημένες προβολές, επιτρέποντας την ταυτόχρονη σύλληψη ποικίλων γλωσσικών σχέσεων---συντακτικών εξαρτήσεων, σημασιολογικών συσχετισμών και μοτίβων αναφοράς. Τα αποτελέσματα συνενώνονται και προβάλλονται γραμμικά. Κάθε μπλοκ Transformer περιλαμβάνει επίσης ένα feed-forward network (δίκτυο πρόσθιας τροφοδότησης), κανονικοποίηση επιπέδου (layer normalization) και συνδέσεις υπολοίπου (residual connections).

Το Positional Encoding (Κωδικοποίηση Θέσης) παρέχει πληροφορία σειράς στα tokens, καθώς ο Transformer δεν διαθέτει εγγενή ακολουθιακή μεροληψία. Η αρχική πρόταση χρησιμοποιεί ημιτονοειδείς συναρτήσεις, ενώ σύγχρονα μοντέλα εφαρμόζουν μαθημένες ή σχετικές κωδικοποιήσεις θέσης, συμπεριλαμβανομένων των Rotary Position Embeddings (RoPE) και ALiBi. Τα σύγχρονα μοντέλα έχουν διαφοροποιηθεί σε τρεις αρχιτεκτονικές οικογένειες: μοντέλα μόνο κωδικοποιητή (encoder-only, π.χ. BERT), μοντέλα μόνο αποκωδικοποιητή (decoder-only, π.χ. GPT, LLaMA), και μοντέλα κωδικοποιητή-αποκωδικοποιητή (encoder-decoder, π.χ. T5, BART).

\subsubsection*{BERT και Μοντέλα Κωδικοποιητή}

Το BERT (Bidirectional Encoder Representations from Transformers) εισήγαγε μια αλλαγή παραδείγματος στην Επεξεργασία Φυσικής Γλώσσας (ΕΦΓ), αποδεικνύοντας ότι η αμφίδρομη προεκπαίδευση κωδικοποιητών Transformer μπορεί να δημιουργήσει πλούσιες πλαισιοποιημένες αναπαραστάσεις. Χρησιμοποιεί δύο στόχους προεκπαίδευσης: Μοντελοποίηση Μασκοφόρου Γλώσσας (Masked Language Modeling -- MLM), όπου κρύβεται τυχαία το 15\% των tokens και το μοντέλο μαθαίνει να τα προβλέψει αξιοποιώντας αμφίδρομο πλαίσιο, και Πρόβλεψη Επόμενης Πρότασης (Next Sentence Prediction -- NSP), που εκπαιδεύει για κατανόηση ακολουθιακών σχέσεων μεταξύ προτάσεων. Σημαντικές παραλλαγές περιλαμβάνουν τα RoBERTa (αφαίρεση NSP, μεγαλύτερα batches), DistilBERT (απόσταξη γνώσης για 60\% μείωση παραμέτρων), και ModernBERT (2024), που εισάγει RoPE, Flash Attention, και εκπαίδευση σε 2 τρισεκατομμύρια tokens.

Τα μοντέλα Encoder διαδραματίζουν κρίσιμο ρόλο στα σύγχρονα συστήματα ανάκτησης, παράγοντας πυκνές διανυσματικές αναπαραστάσεις (vector embeddings) που αποτυπώνουν σημασιολογικό νόημα. Η αρχιτεκτονική Bi-Encoder κωδικοποιεί Query και Document ανεξάρτητα, επιτρέποντας αποτελεσματική ανάκτηση μέσω Ομοιότητας Συνημιτόνου (Cosine Similarity). Αντίθετα, ο Διασταυρούμενος Κωδικοποιητής (Cross-Encoder) επεξεργάζεται ταυτόχρονα το ζεύγος Query-Document, παρέχοντας υψηλότερη ακρίβεια αλλά μεγαλύτερο υπολογιστικό κόστος. Η συνδυαστική αρχιτεκτονική Διπλού Κωδικοποιητή (Bi-Encoder -- γρήγορη ανάκτηση) + Διασταυρούμενου Κωδικοποιητή (Cross-Encoder -- Ανακατάταξη/Reranking) αποτελεί βέλτιστη πρακτική και χρησιμοποιείται στο σύστημά μας.

\subsubsection*{Μοντέλα Αιχμής και Ανοιχτού Κώδικα}

Το τοπίο των Μεγάλων Γλωσσικών Μοντέλων (LLMs) περιλαμβάνει τόσο ιδιωτικά μοντέλα αιχμής (frontier models) όσο και εναλλακτικές ανοιχτού κώδικα. Τα ιδιωτικά μοντέλα αιχμής περιλαμβάνουν τη σειρά GPT (OpenAI) που πρωτοπόρησε στο αυτοπαλίνδρομο παράδειγμα μεγάλης κλίμακας, και τη σειρά Claude (Anthropic) που δίνει έμφαση στην ασφάλεια μέσω Συνταγματικής Τεχνητής Νοημοσύνης (Constitutional AI) και Ενισχυτικής Μάθησης από Ανθρώπινη Ανατροφοδότηση (RLHF). Η αρχική ανάπτυξη του συστήματός μας χρησιμοποίησε τα Claude Sonnet 4.5 (υψηλή ποιότητα) και Claude Haiku 4 (ταχεία πρωτοτυποποίηση) μέσω AWS Bedrock, πριν τη μετάβαση στο GPT-5.2 (OpenAI) για το τελικό σύστημα. Στα ανοιχτού κώδικα, τα LLaMA (Meta), Mistral, Qwen, DeepSeek και ανοιχτά μοντέλα ενσωματωμάτων (π.χ. all-MiniLM-L6-v2, BGE) παρέχουν ανταγωνιστική ποιότητα ζωτικής σημασίας για την αναπαραγωγιμότητα στην ακαδημαϊκή έρευνα.

\subsubsection*{Παραγωγή Δομημένης Εξόδου}

Μια κρίσιμη πρόκληση στην εφαρμογή Μεγάλων Γλωσσικών Μοντέλων (LLMs) σε εργασίες εξαγωγής πληροφοριών είναι η διασφάλιση ότι η έξοδος συμμορφώνεται με προκαθορισμένα σχήματα. Η αφελής ανάλυση ελεύθερου κειμένου εισάγει αποτυχίες JSON, ελλιπή πεδία και ασυνέπειες τύπου. Η μετάβαση στη γραμμικά περιορισμένη αποκωδικοποίηση (constrained decoding) εξαλείφει ολόκληρη κατηγορία σφαλμάτων. Το πλαίσιο Pydantic-AI παρέχει ανεξάρτητη από μοντέλο (model-agnostic) επιβολή σχήματος μέσω Python dataclasses, εξασφαλίζοντας ασφάλεια τύπων, επικύρωση αριθμημένων τιμών (Literal types), αυτόματη μετατροπή JSON schema, και πλήρη κάλυψη πεδίων κατά τη διάρκεια εκτέλεσης.


\subsection*{Μηχανική Προτροπών (Prompt Engineering)}
 
 Η μηχανική προτροπών (Prompt Engineering) αποτελεί βασικό εργαλείο αξιοποίησης Μεγάλων Γλωσσικών Μοντέλων (LLMs) χωρίς μικρο-ρύθμιση (fine-tuning). Εξετάζουμε τεχνικές «zero-shot», «one-shot» και «few-shot», καθώς και την Αλυσίδα Σκέψης (Chain-of-Thought -- CoT), η οποία βελτιώνει σημαντικά την ικανότητα συλλογισμού των Μεγάλων Γλωσσικών Μοντέλων (LLMs) επιτρέποντάς τους να αρθρώσουν ενδιάμεσα βήματα. Πέραν του περιεχομένου, η δομική μορφοποίηση της προτροπής επηρεάζει σημαντικά τη λήψη πληροφοριών.

Η μορφοποίηση XML χρησιμοποιεί ιεραρχικές ετικέτες (π.χ. \texttt{<role>}, \texttt{<extraction\_ontology>}, \texttt{<output\_format>}) που δημιουργούν ρητά δομικά όρια. Πρόσφατη δουλειά τυποποιεί τη δομημένη προτροπή μέσω XML ως αλληλεπίδραση περιορισμένη από γραμματική. Αντίστροφα, η μορφοποίηση Markdown αξιοποιεί ελαφρύτερη σύνταξη με κεφαλίδες, λίστες κουκκίδων και μπλοκ κώδικα. Εμπειρικά, τα μοντέλα Claude αποδίδουν 12\% καλύτερα με XML, ενώ τα μοντέλα GPT αποδίδουν 8\% καλύτερα με Markdown. Η υποστήριξη διπλής μορφοποίησης στο σύστημά μας επέτρεψε ομαλή μετάβαση μεταξύ παρόχων Μεγάλων Γλωσσικών Μοντέλων (LLMs) κατά τη διάρκεια ανάπτυξης.


\subsection*{Πρακτορικά Συστήματα και Ανάκτηση Ενισχυμένη με Παραγωγή}

\subsubsection*{Πράκτορες Βασισμένοι σε LLMs}

Ένας πράκτορας βασισμένος σε Μεγάλα Γλωσσικά Μοντέλα (LLM-based agent) επεκτείνει τις δυνατότητες ενός αυτόνομου γλωσσικού μοντέλου εξοπλίζοντάς τον με βρόχο «αντίληψης-συλλογισμού-ενέργειας». Αντί να παράγει απλά κείμενο, ένας πράκτορας μπορεί να χρησιμοποιεί εργαλεία (αναζήτηση, υπολογισμούς, APIs), να διατηρεί μνήμη μεταξύ αλληλεπιδράσεων, και να εκτελεί πολύβηματες αποστολές αυτόνομα. Το πλαίσιο ReAct (Reasoning and Acting) τυποποίησε τη διεμπλοκή του Chain-of-Thought με χρήση εργαλείων σε κύκλο σκέψη-ενέργεια-παρατήρηση (thought-act-observation). Το Self-Refine (Αυτο-Βελτίωση) εισάγει επαναληπτικό βρόχο βελτίωσης σε τρία στάδια: παραγωγή αρχικής εξόδου, κριτική (Critique), και βελτίωση βάσει ανατροφοδότησης.

εμποδίζοντας την παραγωγή μη έγκυρων δεικτών.

\subsubsection*{Πολυπρακτορικά Συστήματα}

Τα πολυπρακτορικά συστήματα (Multi-agent Systems) αναπτύσσουν εξειδικευμένους πράκτορες που συνεργάζονται ή ανταγωνίζονται. Τα πρότυπα συνεργασίας περιλαμβάνουν: ακολουθιακό αγωγό (pipeline), παράλληλη ψηφοφορία (voting), αντιδικιστική συζήτηση (debate), και ιεραρχική ενορχήστρωση (orchestration). Ένας τεκμηριωμένος περιορισμός είναι η «μεροληψία επιβεβαίωσης» (confirmation bias)---η τάση «κλειδώματος» σε αρχική ερμηνεία. Οι πολυπρακτορικές αρχιτεκτονικές αντιμετωπίζουν αυτό μέσω αναγκαστικής ποικιλομορφίας προοπτικών, με κάθε πράκτορα να αναλύει ανεξάρτητα χωρίς πρόσβαση στις εξόδους των υπολοίπων.

\subsubsection*{Πλαίσια Ενορχήστρωσης}

Το \textbf{LangGraph} παρέχει πλαίσιο κατασκευής πολυπρακτορικών εφαρμογών χρησιμοποιώντας κατευθυνόμενους γράφους, υποστηρίζοντας τυποποιημένη κατάσταση, υπό συνθήκη δρομολόγηση, παράλληλη εκτέλεση μέσω \texttt{asyncio}, και μόνιμη αποθήκευση. Αν και αρχικά χρησιμοποιήσαμε LangGraph για το σύστημα ``Νομοθέτης-Δικαστής'' του S2, η τελική αρχιτεκτονική αντικαθιστά τη γραφική ροή με ασύγχρονη παράλληλη εκτέλεση μέσω \texttt{asyncio.gather}. Το \textbf{Pydantic-AI} συμπληρώνει το LangGraph παρέχοντας ανεξάρτητο-μοντέλου (model-agnostic) επίπεδο αφαίρεσης πρακτόρων, με τυποποιημένη δομημένη έξοδο, ένεση εξαρτήσεων, ενσωμάτωση εργαλείων, και υποστήριξη πολλαπλών παρόχων Μεγάλων Γλωσσικών Μοντέλων (LLMs) μέσω ενιαίας διεπαφής.

\subsubsection*{Ανάκτηση Ενισχυμένη με Παραγωγή (RAG)}
 
 Η Ανάκτηση Ενισχυμένη με Παραγωγή (Retrieval-Augmented Generation -- RAG) αντιμετωπίζει θεμελιώδη περιορισμό των Μεγάλων Γλωσσικών Μοντέλων (LLMs): η γνώση τους είναι ``παγωμένη'' κατά τη χρονική στιγμή εκπαίδευσης. Ένας τυπικός αγωγός RAG περιλαμβάνει τρία στάδια: ευρετηρίαση (indexing) εγγράφων σε πυκνά διανυσματικά ενσωματώματα μέσω μοντέλων ενσωμάτωσης, ανάκτηση (retrieval) των σημασιολογικά πιο παρόμοιων εγγράφων μέσω ομοιότητας συνημιτόνου, και παραγωγή (generation) με ενσωματωμένο πλαίσιο. Οι βάσεις διανυσματικών δεδομένων (vector databases), όπως το ChromaDB, παρέχουν εξειδικευμένη υποδομή αποθήκευσης και ανάκτησης για υψηλοδιάστατα ενσωματώματα, υλοποιώντας ευρετήρια HNSW ή IVF-Flat για αποδοτική αναζήτηση πλησιέστερου γείτονα.

Προηγμένες στρατηγικές ανάκτησης περιλαμβάνουν: Μέγιστη Οριακή Σχετικότητα (MMR), στρωματοποιημένη δειγματοληψία, αντιπαραβολική ανάκτηση (contrastive retrieval) με δύσκολα αρνητικά παραδείγματα, και ανακατάταξη μέσω cross-encoder. Η Ανακατάταξη (Reranking) εφαρμόζει ακριβέστερη βαθμολόγηση στα αρχικά αποτελέσματα διπλού κωδικοποιητή (bi-encoder), βελτιώνοντας 20--35\% την ακρίβεια ανάκτησης.

\subsubsection*{Αυτοματοποιημένη Βελτιστοποίηση Προτροπών}

Το Prompt Optimization αντιμετωπίζει τον σχεδιασμό προτροπών ως πρόβλημα αναζήτησης. Εξελικτικές προσεγγίσεις όπως το EvoPrompt συνδέουν Μεγάλα Γλωσσικά Μοντέλα (LLMs) με εξελικτικούς αλγορίθμους, ενώ η μέθοδος OPRO χρησιμοποιεί Μεγάλα Γλωσσικά Μοντέλα (LLMs) ταυτόχρονα ως βελτιστοποιητή και γεννήτρια λύσεων. Το πλαίσιο DSPy προσφέρει δηλωτική βελτιστοποίηση με αυτόματη μεταγλώττιση. Το GEPA (Genetic Evolutionary Prompt Algorithm), που χρησιμοποιείται στη δική μας εργασία, επεκτείνει αυτές τις προσεγγίσεις με αναστοχαστική μετάλλαξη βασισμένη σε Μεγάλο Γλωσσικό Μοντέλο (LLM), πλούσια διαγνωστική βαθμολόγηση μέσω μηχανισμού «Trojan Horse», και φασική βελτιστοποίηση εξαρτημάτων.


\subsection*{Σχετική Βιβλιογραφία}

\subsection*{Σχετική Βιβλιογραφία}

Η παρούσα εργασία τοποθετείται στη τομή της Επεξεργασίας Φυσικής Γλώσσας, της Υπολογιστικής Κοινωνικής Επιστήμης και της Τεχνητής Νοημοσύνης.

\subsubsection*{Ανίχνευση Συνωμοσιών στην ΕΦΓ}

Οι πρώτες εργασίες για τον συνωμοσιακό λόγο στο Reddit εισήγαγαν αφηγηματικά μοτίβα που συσχετίζονται με συνωμοσιακά στοιχεία \cite{online-discussions} και απέδειξαν ότι η συνωμοσιακή σκέψη εκδηλώνεται μέσω ανιχνεύσιμων ψυχογλωσσικών σημάτων \cite{Klein2019Pathways}. Πέρα από το Reddit, η ευρύτερη βιβλιογραφία παραπληροφόρησης (Misinformation) παρήγαγε σύνολα δεδομένων αναφοράς όπως το LIAR \cite{wang2017liar} και το Propaganda Techniques Corpus \cite{dasanmartino2019finegrained}, τα οποία καθιέρωσαν ότι η ανίχνευση ωφελείται από την αποσύνθεση του προβλήματος σε ερμηνεύσιμα Sub-tasks.

\subsubsection*{Ψυχογλωσσική Μοντελοποίηση}

Η ψυχογλωσσική προσέγγιση έχει τις ρίζες της στη λεξική ανάλυση βάσει λεξικών. Το πλαίσιο LIWC \cite{pennebaker2015liwc} έχει χρησιμοποιηθεί ευρέως για την ποσοτικοποίηση ψυχολογικών διαστάσεων, συμπεριλαμβανομένων δεικτών βεβαιότητας και αιτιακής γλώσσας. Μελέτες δείχνουν ότι ο συνωμοσιακός λόγος παρουσιάζει σημαντικά αυξημένα ποσοστά αιτιακής γλώσσας (``επειδή'', ``επομένως'') και δεικτών βεβαιότητας (``πάντα'', ``σίγουρα'') \cite{language-of-conspiracy-theories}. Ο Εγκληματολογικός Αναλυτής μας λειτουργικοποιεί ένα υποσύνολο αυτών ως ντετερμινιστικά σήματα.

\subsubsection*{Προσεγγίσεις Βασισμένες σε LLMs}

Πρόσφατες αξιολογήσεις δείχνουν ότι η ανίχνευση συνωμοσιών με βάση Μεγάλα Γλωσσικά Μοντέλα (LLMs) συχνά βασίζεται σε θεματικές συντομεύσεις και δυσκολεύεται με την αφηγηματική αμφισημία \cite{pustet-etal-2024-detection}. Οι ταξινομητές μονού περάσματος τείνουν να δεσμεύονται νωρίς και να υποβαθμίζουν ενδείξεις στάσης, οδηγώντας σε συστηματικά ψυχολογικά θετικά όταν η θεματική συζήτηση συγχέεται με την υιοθέτηση---μια αποτυχία που ονομάζουμε \textit{Παγίδα του Ρεπόρτερ (Reporter Trap)}.

\subsubsection*{Πολυπρακτορικά Συστήματα (Multi-agent Systems)}

Η ενορχήστρωση πολλαπλών πρακτόρων Μεγάλων Γλωσσικών Μοντέλων (LLMs) έχει κερδίσει έδαφος. Πλαίσια συζήτησης έχουν δείξει βελτιωμένο συλλογισμό μέσω δομημένης διαφωνίας \cite{agent1, agent3}. Το έργο μας επεκτείνει αυτά τα παραδείγματα εισάγοντας την πρώτη πρακτορική μέθοδο που συνδυάζει αυτο-βελτίωση για εξαγωγή εύρους με αντιδικιστική διαβούλευση συμβουλίου για ταξινόμηση στάσης. Το ChatEval \cite{chan2024chateval} έδειξε ότι η πολυπρακτορική αξιολόγηση παράγει πιο βαθμονομημένες βαθμολογίες ποιότητας, ενώ το Camel \cite{li2023camel} ανέδειξε τη δύναμη των ρόλων (role-playing).

\subsubsection*{Εξαγωγή Εύρους (Span Extraction) και Εξαγωγή Πληροφορίας (Information Extraction)}
 
 Η εξαγωγή εύρους σε επίπεδο χαρακτήρων θέτει μοναδικές προκλήσεις για τα Μεγάλα Γλωσσικά Μοντέλα (LLMs), τα οποία υπερέχουν στον σημασιολογικό συλλογισμό αλλά είναι εύθραυστα στην ακριβή τοπικοποίηση \cite{fu2024struggle}. Προηγούμενες εργασίες έχουν τεκμηριώσει ``ψευδαισθητικά εύρη'' (hallucinated spans). Η υβριδική αρχιτεκτονική μας αντιμετωπίζει αυτό αποσυνδέοντας τη σημασιολογική αναγνώριση από τη ντετερμινιστική ευρετηρίαση, διασφαλίζοντας δομική εγκυρότητα ενώ διατηρεί τις ερμηνευτικές ικανότητες του Μεγάλου Γλωσσικού Μοντέλου (LLM).

\subsubsection*{Βελτιστοποίηση Προτροπών (Prompt Optimization)}

Η αυξανόμενη πολυπλοκότητα των αγωγών Μεγάλων Γλωσσικών Μοντέλων (LLMs) οδήγησε σε αυτοματοποιημένες προσεγγίσεις μηχανικής προτροπών. Εξελικτικές στρατηγικές αντιμετωπίζουν την αναζήτηση οδηγιών ως συνδυαστικό πρόβλημα \cite{agrawal2025gepa}. Το πλαίσιο GEPA μας βασίζεται σε θεμέλια όπως το EvoPrompt \cite{guo2024evoprompt} και το OPRO, εισάγοντας καινοτομίες όπως η αναστοχαστική μετάλλαξη και η εναλλαγή συνόλων εκπαίδευσης.


\section*{Σχεδίαση Συστήματος και Μεθοδολογία}

\subsection*{Περιγραφή Εργασίας και Σύνολο Δεδομένων}

Το σύνολο δεδομένων του SemEval-2026 Task 10 περιλαμβάνει 4.800 σχολιασμούς που αφορούν 4.100 μοναδικά σχόλια στο Reddit από περισσότερα από 190 subreddits. Το Υποέργο 1 (S1) αφορά την εξαγωγή δεικτών σε πέντε κατηγορίες: Δράστης (Actor), Ενέργεια (Action), Αποτέλεσμα (Effect), Θύμα (Victim) και Στοιχεία (Evidence). Το Υποέργο 2 (S2) αφορά τη δυαδική ταξινόμηση (συνωμοσία / μη-συνωμοσία).

Η διερευνητική ανάλυση δεδομένων (Exploratory Data Analysis - EDA) αποκάλυψε σημαντικά μοτίβα που καθοδήγησαν τον σχεδιασμό του συστήματος.

\subsubsection*{Κάλυψη Σχολιασμού}

Όπως αναφέρεται στον επίσημο ιστότοπο της εργασίας, υπάρχουν περισσότερα από 4.100 μοναδικά σχόλια Reddit, συμπεριλαμβανομένων 4.800 σχολιασμών συνολικά. Τα περισσότερα σχόλια ($\sim$3.500) έχουν μόνο έναν σχολιασμό, 550 έχουν δύο, και 50 έχουν περισσότερους. Η ακριβής κατανομή της κάλυψης κατηγοριών δεικτών στα σχόλια παρουσιάζεται στο Σχήμα~\ref{fig:greek-num-markers}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/num-of-markers.png}
    \caption{Πλήθος τύπων δεικτών στο σύνολο δεδομένων.}
    \label{fig:greek-num-markers}
\end{figure}

\subsubsection*{Κατανομή Ετικετών}

Το σύνολο δεδομένων θεωρεί δύο σαφείς κλάσεις, \textit{Yes (Conspiracy)} και \textit{No (Not Conspiracy)}, ενώ η κλάση \textit{Can't Tell} καλύπτει αβέβαιες περιπτώσεις. Η κατανομή των ετικετών στα δεδομένα εκπαίδευσης απεικονίζεται στο Σχήμα~\ref{fig:greek-label-distr}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/label-distr.png}
    \caption{Κατανομή ετικετών για την ανίχνευση συνωμοσίας.}
    \label{fig:greek-label-distr}
\end{figure}

Η κατανομή των πέντε τύπων ψυχογλωσσικών δεικτών στα δεδομένα εκπαίδευσης ακολουθεί εκείνη του Σχήματος~\ref{fig:greek-marker-types}. Βάσει αυτού, συμπεραίνουμε ότι οι συνωμοσιακές αφηγήσεις βασίζονται σε ένα μικρό σύνολο επαναλαμβανόμενων ρητορικών λειτουργιών, αλλά καμία μεμονωμένη λειτουργία δεν κυριαρχεί στον λόγο.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/marker-types.png}
    \caption{Συχνότητα ανά τύπο δείκτη.}
    \label{fig:greek-marker-types}
\end{figure}

\subsubsection*{Πυκνότητα Σχολιασμού}

Η πυκνότητα σχολιασμού υποδεικνύει έμμεσα τη δυσκολία σχολιασμού του συνόλου δεδομένων: ένα αραιά σχολιασμένο σύνολο δείχνει ότι τα αποδεικτικά στοιχεία συνωμοσίας είναι σημασιολογικά καλά διαχυμένα στο κείμενο. Πράγματι, αρκετά έγγραφα περιέχουν 0 σχολιασμούς, ενώ τα περισσότερα δεν υπερβαίνουν τους 20. Η μακρά ουρά κατανομής δεικτών ανά έγγραφο που παρουσιάζεται στο Σχήμα~\ref{fig:greek-density} επιβεβαιώνει τη δυσκολία της εργασίας.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/annot-density.png}
    \caption{Πλήθος σχολιασμών δεικτών ανά έγγραφο.}
    \label{fig:greek-density}
\end{figure}

Είναι επίσης χρήσιμο να εμφανίσουμε τις συνεμφανίσεις δεικτών στα δεδομένα εκπαίδευσης, όπως στο Σχήμα~\ref{fig:greek-cooccurrence}, υποδεικνύοντας ότι οι τύποι δεικτών συχνά εμφανίζονται μαζί στα ίδια έγγραφα.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/cooccurrence.png}
    \caption{Συνεμφανίσεις τύπων δεικτών.}
    \label{fig:greek-cooccurrence}
\end{figure}

Η υψηλή αυτο-συνεμφάνιση των δεικτών \textit{Action} και \textit{Actor} δείχνει ότι πολλά έγγραφα περιγράφουν πολλαπλές ενέργειες και πολλαπλούς πράκτορες. Η ισχυρή συνεμφάνιση μεταξύ \textit{Action} και \textit{Actor} τονίζει περαιτέρω την απόδοση αυτενέργειας ως κεντρική οργανωτική αρχή.

\subsubsection*{Ανάλυση Επικάλυψης Εύρους (Span Overlap Analysis)}

Για να ποσοτικοποιήσουμε τον βαθμό επικάλυψης εύρους πέραν της δυαδικής συνεμφάνισης, υπολογίζουμε το μέσο Intersection over Union (IoU) σε επίπεδο χαρακτήρων για όλα τα επικαλυπτόμενα ζεύγη εύρους, που παρουσιάζεται στο Σχήμα~\ref{fig:greek-iou-matrix}. Η υψηλότερη επικάλυψη κατά ζεύγη συμβαίνει μεταξύ \textsc{Actor} και \textsc{Victim} (μέσο IoU$=$0.65), αντανακλώντας το συχνό ρητορικό μοτίβο όπου το κατηγορούμενο μέρος πλαισιώνεται ταυτόχρονα ως ανταγωνιστής και θιγόμενη οντότητα.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/mean-iou-matrix.png}
    \caption{Μέση IoU επικαλυπτόμενων εύρων μεταξύ ζευγών τύπων δεικτών.}
    \label{fig:greek-iou-matrix}
\end{figure}

\subsubsection*{Κατανομή Δεικτών ανά Subreddit (Marker Distribution per Subreddit)}

Διερευνάμε το ποσοστό σχολιασμένων δεικτών ανά subreddit, που απεικονίζεται στο Σχήμα~\ref{fig:greek-subreddits}. Τα subreddits παρουσιάζουν αξιοσημείωτες διαφορές όσον αφορά τον κυρίαρχο τύπο δείκτη. Για παράδειγμα, το \textit{Action} εμφανίζεται αρκετά σταθερό, ενώ ο ρόλος του \textit{Actor} γίνεται πιο εξέχων σε κοινότητες όπως το Israel\_Palestine.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/pct-subreddit.png}
    \caption{Κατανομή τύπων δεικτών ανά Subreddit.}
    \label{fig:greek-subreddits}
\end{figure}

\subsubsection*{Ανάλυση Θέσης Εύρους (Span Position Analysis)}

Το Σχήμα~\ref{fig:greek-span-position} εμφαίνιζει την εκτίμηση πυκνότητας πυρήνα (KDE) των κανονικοποιημένων θέσεων κέντρου εύρους εντός των εγγράφων. Οι δείκτες \textsc{Actor} συγκεντρώνονται προς την αρχή των εγγράφων (διάμεση θέση$=$0.09), συνεπείς με αφηγηματικά ανοίγματα που καθιερώνουν αυτενέργεια. Αντίθετα, οι δείκτες \textsc{Effect} κορυφώνονται αργότερα (διάμεση θέση$=$0.43).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/span-position-analysis.png}
    \caption{Κανονικοποιημένη θέση δεικτών εντός εγγράφων (0$=$αρχή, 1$=$τέλος).}
    \label{fig:greek-span-position}
\end{figure}

\subsubsection*{Λεξικά Σήματα (Lexical Signals)}

Τα κείμενα συνωμοσίας εμφανίζουν 32\% υψηλότερο ρυθμό απολυταρχικής γλώσσας (absolutist language) και 18\% χαμηλότερη χρήση επιφυλάξεων (hedges) σε σχέση με τα μη-συνωμοσιακά κείμενα ($p < 0.001$, Cliff's $\delta > 0.4$). Αυτό επιβεβαιώνει την ψυχολογική θεωρία ότι η συνωμοσιακή σκέψη χαρακτηρίζεται από δογματισμό και έλλειψη αποχρώσεων.

\subsection*{Αρχιτεκτονική Συστήματος (System Architecture)}

Υλοποιούμε μια πρακτορική ροή εργασίας δύο σταδίων, σχεδιασμένη να αντιμετωπίζει τις ξεχωριστές προκλήσεις της εξαγωγής εύρους και της ταξινόμησης εγγράφων. Το Σχήμα~\ref{fig:greek-arch-tikz} παρουσιάζει τη συνολική αρχιτεκτονική.

\begin{figure}[H]
    \centering
    \scriptsize
    \resizebox{\linewidth}{!}{
        \begin{tikzpicture}[
            node distance=0.6cm and 0.4cm,
            >={Stealth[length=3pt]},
            % Styles
            inputbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
                    minimum width=1.4cm, align=center, font=\scriptsize\sffamily, fill=gray!10},
            ragbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
                    minimum width=1.4cm, align=center, font=\scriptsize\sffamily, fill=green!10},
            llmbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
                    minimum width=1.4cm, align=center, font=\scriptsize\sffamily, fill=blue!10},
            detbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
                    minimum width=1.4cm, align=center, font=\scriptsize\sffamily, fill=orange!15},
            outputbox/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
                    minimum width=1.4cm, align=center, font=\scriptsize\sffamily, thick},
            arrow/.style={->, thick, black!70, rounded corners=2pt}, dasharrow/.style={->,
                    thick, dashed, black!50, rounded corners=2pt},
            grouplab/.style={font=\bfseries\scriptsize\sffamily, text=black!70},
            labeltext/.style={font=\tiny\sffamily, text=black!60, midway, fill=white, inner
                    sep=1pt}, ]

            % === INPUT ===
            \node[inputbox] (input) {Document};

            % === S1 PIPELINE (Top Row) ===
            \node[ragbox, right=0.8cm of input] (s1rag) {Stratified\\Few-shot\\Retrieval};
            \node[llmbox, right=0.5cm of s1rag] (gen) {DD-CoT\\Generator};
            \node[llmbox, right=0.5cm of gen] (critic) {Enhanced\\Critic};
            \node[llmbox, right=0.5cm of critic] (refiner) {Refiner\\Agent};
            \node[detbox, right=0.5cm of refiner] (verifier) {Deterministic\\Verifier};
            \node[outputbox, right=0.5cm of verifier] (s1out) {S1 Spans};

            % S1 Connections
            \draw[arrow] (input) -- (s1rag);
            \draw[arrow] (s1rag) -- (gen);
            \draw[arrow] (gen) -- (critic);
            \draw[arrow] (critic) -- (refiner);
            \draw[arrow] (refiner) -- (verifier);
            \draw[arrow] (verifier) -- (s1out);

            % S1 Group Label
            \begin{scope}[on background layer]
                \node[draw=blue!40, fill=blue!3, rounded corners=4pt, dashed,
                fit=(s1rag)(gen)(critic)(refiner)(verifier)(s1out),
                inner sep=8pt, label={[grouplab]above:{S1: Marker Extraction (Self-Refine + DD-CoT)}}] (s1group) {};
            \end{scope}

            % === S2 PIPELINE (Bottom Row) ===
            \node[ragbox, below=2.2cm of s1rag] (s2rag) {Contrastive\\Few-shot\\Retrieval};
            \node[detbox, right=0.5cm of s2rag] (forensic) {Forensic\\Profiler};

            % PARALLEL COUNCIL (Strict 2x2 Grid)
            \node[llmbox, right=0.8cm of forensic, yshift=0.6cm] (pros) {Prosecutor};
            \node[llmbox, right=0.5cm of pros] (literal) {Literalist\\{\tiny $\tau$=0.4}};

            \node[llmbox, right=0.8cm of forensic, yshift=-0.6cm] (defense) {Defense};
            \node[llmbox, right=0.5cm of defense] (profiler) {Stance\\Profiler};

            % Judge
            \node[llmbox, right=0.6cm of $(literal.east)!0.5!(profiler.east)$] (judge) {Calibrated\\Judge};
            \node[outputbox, right=0.5cm of judge] (verdict) {Final\\Verdict};

            % Fork (Clean Distribution)
            \coordinate[right=0.4cm of forensic] (fork_point);
            \draw[thick, black!70] (forensic.east) -- (fork_point);
            \draw[arrow] (fork_point) |- (pros.west);
            \draw[arrow] (fork_point) |- (defense.west);

            % S2 Connections
            \draw[arrow] (input.south) |- (s2rag.west);
            \draw[arrow] (input.south) |- ($(s2rag.south) + (0,-0.5)$) -| (forensic.south);

            % S1 Out to Council 
            \draw[arrow] (s1out.south) -- ++(0,-1.5) -| (fork_point);

            % S2RAG to Council (Bypassing Forensic)
            \draw[arrow] (s2rag.north) -- ++(0,0.3) -| (fork_point);

            % Internal Flow
            \draw[arrow] (pros) -- (literal);
            \draw[arrow] (defense) -- (profiler);

            % Merge (Clean Join)
            \draw[arrow] (literal.east) -- ++(0.2,0) |- (judge.west);
            \draw[arrow] (profiler.east) -- ++(0.2,0) |- (judge.west);
            \draw[arrow] (judge) -- (verdict);

            % S2 Group Label
            \begin{scope}[on background layer]
                \node[draw=red!40, fill=red!3, rounded corners=4pt, dashed,
                fit=(s2rag)(forensic)(pros)(defense)(literal)(profiler)(judge)(verdict),
                inner sep=8pt, label={[grouplab]below:{S2: Anti-Echo Chamber (Parallel Council)}}] (s2group) {};
            \end{scope}

        \end{tikzpicture}
    }
    \caption{Επισκόπηση αρχιτεκτονικής συστήματος. \textbf{S1} (πάνω): DD-CoT Self-Refine για εξαγωγή δεικτών. \textbf{S2} (κάτω): Αντι-Ηχοθάλαμος με εγκληματολογική προφίλληση, παράλληλο συμβούλιο, και βαθμονομημένο δικαστή.}
    \label{fig:greek-arch-tikz}
\end{figure}

\subsubsection*{S1: Εξαγωγή Δεικτών μέσω DD-CoT (Marker Extraction via DD-CoT)}

Το Υποέργο 1 (S1) συνδυάζει ένα βρόχο Αυτο-Βελτίωσης (Self-Refine) με Ντετερμινιστικό Επαληθευτή (Deterministic Verifier) (βλ. Σχήμα~\ref{fig:greek-arch-tikz}, πάνω).

Ο σχεδιασμός ρητά διαχωρίζει τη \textit{σημασιολογική αναγνώριση} από την \textit{ευρετηρίαση εύρους}: τα LLMs εκπέμπουν αυτολεξεί συμβολοσειρές δεικτών με ετικέτες, και ένας Ντετερμινιστικός Επαληθευτής (Deterministic Verifier) υπολογίζει τις θέσεις χαρακτήρων.

\paragraph*{Γεννήτρια DD-CoT (DD-CoT Generator).} Η Δυναμική Διακριτική Αλυσίδα Σκέψης (Dynamic Discriminative Chain-of-Thought -- DD-CoT) επεκτείνει τον συλλογισμό CoT με ένα ρητό βήμα \textit{Διάκρισης (Discrimination)}. Για κάθε υποψήφιο Εύρος (Span), ο Generator πρέπει να δηλώσει:
\begin{enumerate}
    \item \textbf{Παράθεση (Quote):} Το ακριβές κείμενο από το έγγραφο (verbatim extraction).
    \item \textbf{Συλλογισμός (Reasoning):} Τεκμηρίωση υπέρ της επιλεγμένης ετικέτας (π.χ. ``Αυτό το τμήμα περιγράφει τον πράκτορα της συνωμοσίας'').
    \item \textbf{Διάκριση (Discrimination):} Αντεπιχείρημα κατά της πιο πιθανής εναλλακτικής ετικέτας (π.χ. ``Δεν είναι Θύμα, καθώς ενεργεί ενεργητικά'').
\end{enumerate}

\paragraph*{Ντετερμινιστικός Επαληθευτής (Deterministic Verifier).} Ο Επαληθευτής (Verifier) λειτουργεί ως το στοιχείο ``Γείωσης'' (Grounding) του συστήματος. Αναζητά το παραγόμενο απόσπασμα στο πρωτότυπο κείμενο χρησιμοποιώντας έναν καταρράκτη πέντε επιπέδων:
\begin{enumerate}
    \item \textbf{Επίπεδο 1 (Ακριβές):} Ακριβής ταύτιση χαρακτήρων.
    \item \textbf{Επίπεδο 2 (Ανεξάρτητο Πεζών-Κεφαλαίων):} Αγνοεί πεζά-κεφαλαία.
    \item \textbf{Επίπεδο 3 (Κανονικοποιημένο):} Αγνοεί σημεία στίξης και λευκά κενά.
    \item \textbf{Επίπεδο 4 (Ασαφές):} Ασαφής ταύτιση Levenshtein (κατώφλι 90\%).
    \item \textbf{Επίπεδο 5 (LCS):} Μέγιστη Κοινή Υποακολουθία (Longest Common Subsequence) για πολύ θορυβώδη κείμενα.
\end{enumerate}
Εάν ένα απόσπασμα αποτύχει σε όλα τα επίπεδα, απορρίπτεται ως ``Ψευδαίσθηση'' (Hallucination), εμποδίζοντας την παραγωγή μη έγκυρων δεικτών.

\paragraph*{Κριτής και Βελτιωτής (Critic και Refiner).} Ένας Πράκτορας Κριτής (Critic agent) ελέγχει τα εξαγόμενα για λογική συνέπεια (π.χ. ``Ένας Δράστης πρέπει να είναι πρόσωπο ή ομάδα''). Εάν εντοπιστούν σφάλματα, ο Βελτιωτής (Refiner) καλείται να διορθώσει την έξοδο, λαμβάνοντας υπόψη την κριτική.

\subsubsection*{S2: Ταξινόμηση μέσω Αντι-Ηχοθαλάμου (Classification via Anti-Echo Chamber)}

Το Υποέργο 2 (S2) στοχεύει συγκεκριμένα στην αποτυχία \textbf{Παγίδα του Ρεπόρτερ (Reporter Trap)}, όπου η θεματική συζήτηση συνωμοσιών συγχέεται με την υιοθέτησή τους (βλ. Σχήμα~\ref{fig:greek-arch-tikz}, κάτω).

Η ταξινόμηση δομείται σε τρία επίπεδα:

\paragraph*{1. Εγκληματολογικός Αναλυτής (Forensic Profiler).} Ένα ντετερμινιστικό σύστημα που εξάγει σήματα πριν ξεκινήσει η διαβούλευση. Υπολογίζει:
\begin{itemize}
    \item \textit{Πυκνότητα Απόδοσης (Attribution Density):} Πόσο συχνά το κείμενο αποδίδει ισχυρισμούς σε τρίτους (π.χ. ``σύμφωνα με τους...''). Υψηλή πυκνότητα υποδεικνύει Αναφορά (Reporting), όχι Υιοθέτηση (Endorsement).
    \item \textit{Βαθμολογία Απολυτότητας (Absolutist Score):} Ποσοστό απολυταρχικών λέξεων (π.χ. ``πάντα'', ``ποτέ'').
    \item \textit{Παράγοντας JAQing (JAQing Factor):} Παρουσία ρητορικών ερωτήσεων (Just Asking Questions).
\end{itemize}
Τα αποτελέσματα παρέχονται ως ``αποδεικτικά στοιχεία'' στο συμβούλιο.

\paragraph*{2. Παράλληλο Συμβούλιο (Parallel Council).} Τέσσερις εξειδικευμένοι Πράκτορες (Agents) αναλύουν το κείμενο ταυτόχρονα, ο καθένας με διαφορετική εντολή (Persona):
\begin{itemize}
    \item \textbf{Εισαγγελέας (Prosecutor):} Αναζητά αποδείξεις συνωμοσίας. Εστιάζει σε λανθάνουσες σημασίες και συνθηματικές λέξεις (dog-whistles).
    \item \textbf{Συνήγορος Υπεράσπισης (Defense Attorney):} Αναζητά αθωωτικές εξηγήσεις (π.χ. σαρκασμό, αναφορά τρίτων, κατάρριψη/debunking).
    \item \textbf{Κυριολεκτικός (Literalist):} Εξετάζει αυστηρά το κείμενο βάσει του ορισμού συνωμοσίας, αγνοώντας τα συμφραζόμενα.
    \item \textbf{Αναλυτής Στάσης (Stance Analyst):} Εστιάζει αποκλειστικά στη στάση του συγγραφέα προς το θέμα (θετική, αρνητική, ουδέτερη).
\end{itemize}

\paragraph*{3. Βαθμονομημένος Δικαστής (Calibrated Judge).} Ένας τελικός Πράκτορας (Agent) συλλέγει τις τέσσερις αναλύσεις και εκδίδει την τελική ετυμηγορία. Χρησιμοποιεί αυστηρούς κανόνες: σε περίπτωση ισοψηφίας (2-2), αποφαίνεται ``Μη Συνωμοσία'' με χαμηλή εμπιστοσύνη, προτιμώντας την αποφυγή Ψευδώς Θετικών (False Positives).


\subsection*{Ψυχογλωσσική Θεμελίωση (Psycholinguistic Grounding)}

Διακριτικό χαρακτηριστικό της προσέγγισής μας αποτελεί η ρητή θεμελίωση στην εξελικτική ψυχολογία και τη θεωρία ψυχογλωσσικής ρητορικής. Οι πέντε κατηγορίες ψυχογλωσσικών δεικτών (Δράστης, Ενέργεια, Αποτέλεσμα, Θύμα, Στοιχεία) θεμελιώνονται σε ευρετικές ανίχνευσης απειλών. Κάθε δείκτης αντιστοιχεί σε ένα στοιχείο της δομής συνωμοσιακής αφήγησης: οι Δράστες (Actors) εκμεταλλεύονται τη γνωστική μεροληψία ανίχνευσης παραγόντων, τα Θύματα (Victims) πυροδοτούν ενστικτώδη αντίδραση προστασίας εσω-ομάδας, και τα Στοιχεία (Evidence) χρησιμοποιούν αυτοσφράγιστη επιστημολογία (``κάνε τη δική σου έρευνα'').

Κρίσιμη καινοτομία αποτελεί ο \textbf{Κανόνας Δομικής Δήλωσης}: δηλώσεις που υποστηρίζουν την ύπαρξη συνωμοσίας ως γεγονός, ακόμη και χωρίς πρωτοπρόσωπη πεποίθηση ή συναισθηματική γλώσσα, αποτελούν \textit{Υιοθέτηση μέσω Δήλωσης} και ταξινομούνται ως συνωμοσία. Αυτός ο κανόνας αντιμετωπίζει τη μη προφανή διάκριση μεταξύ δήλωσης και αναφοράς.


\subsection*{Αντιπαραβολική Ανάκτηση Παραδειγμάτων}

Για το S1, υλοποιούμε στρωματοποιημένη Αντιπαραβολική Δειγματοληψία (Contrastive Sampling) με ισορροπημένα θετικά και αρνητικά παραδείγματα, στρωματοποίηση κατά τύπο δείκτη, και Ανακατάταξη μέσω Διασταυρούμενου Κωδικοποιητή (Cross-Encoder Reranking). Για το S2, εφαρμόζουμε Εξόρυξη Δύσκολων Αρνητικών (Hard Negative Mining): ανακτώμε κείμενα μη-συνωμοσίας που περιέχουν δείκτες S1, εξαναγκάζοντας το μοντέλο να εστιάσει σε ενδείξεις Στάσης (Stance) αντί για λέξεις-κλειδιά θέματος. Το Σχήμα~\ref{fig:greek-rag-tikz} απεικονίζει τη ροή ανάκτησης.

\begin{figure}[H]
    \centering
    \small
    \resizebox{0.85\linewidth}{!}{
        \begin{tikzpicture}[
            node distance=0.5cm and 0.4cm,
            >={Stealth[length=3pt]},
            box/.style={rectangle, draw, rounded corners=2pt, minimum height=0.6cm,
                    align=center, font=\scriptsize\sffamily, inner sep=2pt},
            sbox/.style={box, fill=blue!10, minimum width=2.2cm},
            nbox/.style={box, fill=red!10, minimum width=2.2cm},
            gbox/.style={box, fill=green!10, minimum width=2.8cm},
            arrow/.style={->, thick, black!70},
            ]

            \node[box, fill=gray!15, minimum width=2.5cm] (query) {Input Document};
            \node[box, fill=yellow!15, below=0.5cm of query, minimum width=2.5cm] (chroma) {ChromaDB Embeddings};
            \draw[arrow] (query) -- node[right, font=\tiny\sffamily] {embed} (chroma);

            \node[sbox, below left=0.6cm and 0.3cm of chroma] (s1ret) {Balanced + Type\\Stratification (60\%)};
            \node[nbox, below right=0.6cm and 0.3cm of chroma] (s2ret) {Hard Negatives\\(Non-CT + Markers)};
            \draw[arrow] (chroma.south) -- ++(-0.1,-0.2) -| (s1ret.north);
            \draw[arrow] (chroma.south) -- ++(0.1,-0.2) -| (s2ret.north);

            \node[gbox, below=0.5cm of s1ret] (rerank1) {Cross-Encoder\\Reranking (3$\times$)};
            \node[gbox, below=0.5cm of s2ret] (rerank2) {Reranking (4$\times$)\\+ Filtering};
            \draw[arrow] (s1ret) -- (rerank1);
            \draw[arrow] (s2ret) -- (rerank2);

            \node[box, fill=blue!5, below=0.5cm of rerank1, minimum width=2.2cm] (out1) {S1 Few-Shots};
            \node[box, fill=red!5, below=0.5cm of rerank2, minimum width=2.2cm] (out2) {S2 Precedents};
            \draw[arrow] (rerank1) -- (out1);
            \draw[arrow] (rerank2) -- (out2);

        \end{tikzpicture}
    }
    \caption{Αρχιτεκτονική αντιπαραβολικής ανάκτησης παραδειγμάτων.}
    \label{fig:greek-rag-tikz}
\end{figure}


\subsection*{Βελτιστοποίηση Προτροπών μέσω GEPA}

Τα πρότυπα προτροπών βελτιστοποιήθηκαν μέσω του Γενετικού Εξελικτικού Αλγορίθμου Προτροπών (Genetic Evolutionary Prompt Algorithm -- GEPA), ο οποίος αντιμετωπίζει τη μηχανική προτροπών ως πρόβλημα αναζήτησης στον χώρο οδηγιών φυσικής γλώσσας. Ο αλγόριθμος χρησιμοποιεί τουρνουά επιλογής, σημασιολογική διασταύρωση μέσω LLMs, και αναστοχαστική μετάλλαξη βασισμένη σε ανατροφοδότηση. Η βελτιστοποίηση απέφερε σημαντικές βελτιώσεις: +12.5\% στο F1 του S1 και +27.4\% στην ακρίβεια δύσκολων αρνητικών του S2. Το Σχήμα~\ref{fig:greek-gepa-tikz} απεικονίζει τη ροή βελτιστοποίησης.

\begin{figure}[H]
    \centering
    \resizebox{\linewidth}{!}{
        \small
        \begin{tikzpicture}[
            node distance=0.6cm and 0.5cm,
            >={Stealth[length=3pt]},
            process/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
                    minimum width=1.8cm, align=center, font=\scriptsize\sffamily, fill=blue!10},
            decision/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
                    minimum width=1.8cm, align=center, font=\scriptsize\sffamily, fill=green!10},
            mutation/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
                    minimum width=1.8cm, align=center, font=\scriptsize\sffamily, fill=orange!15},
            output/.style={rectangle, draw, rounded corners=2pt, minimum height=0.7cm,
                    minimum width=1.8cm, align=center, font=\bfseries\scriptsize\sffamily,
                    fill=red!10, thick}, scorer/.style={rectangle, draw, dashed, rounded
                    corners=2pt, minimum height=0.7cm, minimum width=3.8cm, align=left,
                    font=\tiny\sffamily, fill=yellow!10}, arrow/.style={->, thick, black!70,
                    rounded corners=2pt}, dasharrow/.style={->, thick, dashed, black!50, rounded
                    corners=2pt}, ]

            % === TOP ROW (Main Flow) ===
            \node[process] (pop) {Πληθυσμός\\(20--30 προτροπές)};
            \node[process, right=0.6cm of pop] (eval) {Αξιολόγηση (MLflow)\\Εναλλαγή Εκπαίδευσης/Ανάπτυξης};
            \node[decision, right=0.6cm of eval] (select) {Επιλογή +\\Διασταύρωση};

            % === SCORING DETAILS (Annotation) ===
            \node[scorer, above=0.3cm of eval] (score_detail) {
                \textbf{Προσαρμοσμένος Βαθμολογητής (Trojan Horse Passthrough)}\\
                \textbf{S1:} $F_{\beta=2}$ (Μεροληπτικό στην Ανάκληση) + Ανατροφοδότηση Ορίων\\
                \textbf{S2:} Συναίνεση Κλίσης (Gradient Consensus) + Βαθμονόμηση
            };
            \draw[dashed, black!40] (eval.north) -- (score_detail.south);

            % === BOTTOM ROW (Feedback & Output) ===
            \node[mutation, below=0.8cm of select] (mutate) {Αναστοχαστική\\Μετάλλαξη LLM};
            \node[output, below=0.8cm of pop] (best) {Βέλτιστη Προτροπή\\($+$4.2\% $F_1$)};

            % === CONNECTIONS ===
            % Main sequence
            \draw[arrow] (pop) -- (eval);
            \draw[arrow] (eval) -- (select);
            \draw[arrow] (select) -- (mutate);

            % Feedback Loop
            \draw[dasharrow] (mutate.south) -- ++(0,-0.4)
            -| ([xshift=-0.4cm]best.west)
            |- (pop.west)
            node[pos=0.25, above, font=\tiny\sffamily, text=black!60] {Επόμενη Γενιά (40--80 δοκιμές)};

            % Best Prompt Extraction
            \draw[arrow] (eval.south west) -- ++(-0.2,-0.2) |- (best.east);

        \end{tikzpicture}
    }
    \caption{Ροή βελτιστοποίησης GEPA. Ένας πληθυσμός υποψήφιων προτροπών εξελίσσεται μέσω αξιολόγησης, επιλογής, διασταύρωσης, και αναστοχαστικής μετάλλαξης σε 40--80 γενεές.}
    \label{fig:greek-gepa-tikz}
\end{figure}


\subsection*{Πορεία Ανάπτυξης}

Η ανάπτυξη του συστήματος καλύπτει πέντε μήνες (Σεπτέμβριος 2025--Ιανουάριος 2026) μέσω 42 git commits, σε έξι φάσεις: (1) Κατανόηση δεδομένων και EDA, (2) Δημιουργία βασικής γραμμής με AWS Bedrock και Claude Sonnet 4.5, (3) Εντατική μηχανική προτροπών, (4) Μετάβαση σε πρακτορική αρχιτεκτονική με Pydantic-AI και LangGraph, (5) Αυτοματοποιημένη βελτιστοποίηση μέσω GEPA, και (6) Ενοποίηση αρχιτεκτονικής και μετάβαση σε GPT-5.2.

Η συστηματική ανάλυση σφαλμάτων αποκάλυψε τέσσερις κρίσιμες αποτυχίες: σύγχυση ετικετών (Action $\leftrightarrow$ Effect), Hallucinated Spans, Reporter Trap, και μεροληψία ακολουθιακής συζήτησης. Κάθε αποτυχία οδήγησε σε συγκεκριμένη αρχιτεκτονική λύση: DD-CoT, Deterministic Verifier, Anti-Echo Chamber, και παράλληλη εκτέλεση.


\section*{Αποτελέσματα}

\subsection*{Κύρια Αποτελέσματα}

Η πλήρης πρακτορική αγωγός διπλασιάζει το μακρο-F1 του S1 (από 0.12 σε 0.24) και βελτιώνει το μακρο-F1 του S2 κατά 49\% (από 0.53 σε 0.79) σε σχέση με τη βασική γραμμή μηδενικής προτροπής του GPT-5.2. Όλες οι βελτιώσεις προκύπτουν αποκλειστικά από τη μηχανική προτροπών και τον πρακτορικό σχεδιασμό ροής εργασίας, χωρίς τελειοποίηση μοντέλου (fine-tuning) ή πρόσθετα δεδομένα εκπαίδευσης. Η σταδιακή συνεισφορά κάθε στοιχείου είναι: αντιπαραβολική ανάκτηση (+0.05 F1 στο S1, +0.10 F1 στο S2), βρόχος αυτο-βελτίωσης / συμβούλιο (+0.05 στο S1, +0.11 στο S2), και πλήρης αγωγός με GEPA (+0.02 στο S1, +0.05 στο S2).

\subsection*{Ανάλυση S1: Εξαγωγή Δεικτών}

Η ανάλυση ανά κατηγορία δείκτη αποκαλύπτει σημαντικές διαφορές: ο \textsc{Δράστης} (Actor) επιτυγχάνει το υψηλότερο F1 (0.29), πιθανώς διότι οι αναφορές σε δράστες είναι γλωσσικά ρητές (κύρια ονόματα, οργανισμοί). Η \textsc{Ενέργεια} (Action, F1=0.26) και το \textsc{Θύμα} (Victim, F1=0.22) ακολουθούν, ενώ το \textsc{Αποτέλεσμα} (Effect, F1=0.19) και τα \textsc{Στοιχεία} (Evidence, F1=0.20) αποτελούν τις πιο δύσκολες κατηγορίες, λόγω σημασιολογικής διάχυσης μεταξύ προτάσεων.

Ο Ντετερμινιστικός Επαληθευτής αποδείχθηκε κρίσιμος: το 78.3\% των εγκύρων εύρων επιλύονται στο πρώτο επίπεδο (ακριβής αντιστοίχιση), 12.1\% στο δεύτερο (χωρίς διάκριση πεζών-κεφαλαίων), 5.8\% στο τρίτο (κανονικοποίηση), και μόνο 3.8\% απαιτεί ασαφή (2.9\%) ή ακολουθιακή ανάκτηση LCS (0.9\%). Το υψηλό ποσοστό ακριβούς αντιστοίχισης επιβεβαιώνει ότι η οδηγία αυτολεξεί εξαγωγής του Παραγωγού DD-CoT λειτουργεί αποτελεσματικά.

\subsection*{Ανάλυση S2: Ανίχνευση Συνωμοσίας}

Η αρχιτεκτονική Anti-Echo Chamber μείωσε το ποσοστό False Positives σε μη-συνωμοσιακά κείμενα κατά 65\% (από 32\% σε 11\%). Η προειδοποίηση Attribution Density του Forensic Profiler ενεργοποιείται στο 89\% των False Positives τύπου ``Reporting'', παρέχοντας ισχυρό πρώτο φίλτρο.

Η ανάλυση μοτίβων ψηφοφορίας του Συμβουλίου αποκαλύπτει: ομόφωνη συμφωνία σε 67\% των κειμένων (ακρίβεια 94\%), διαίρεση 3-1 σε 24\% (ακρίβεια 79\%), και διαίρεση 2-2 σε 9\% (ακρίβεια 61\%). Οι περιπτώσεις 2-2 αντιπροσωπεύουν πραγματικά αμφίσημα κείμενα όπου και οι ανθρώπινοι σχολιαστές εμφανίζουν αυξημένη διαφωνία.

\subsection*{Ποιοτική Ανάλυση Σφαλμάτων}

Εξετάζοντας τα 50 σφάλματα υψηλότερης εμπιστοσύνης ανά υποέργο, εντοπίσαμε συγκεκριμένες αποτυχίες αλλά και επιτυχίες του αρχιτεκτονικού σχεδιασμού.

\paragraph*{Επιτυχία: Διαχωρισμός Αυτενέργειας (S1).}
Το DD-CoT βελτίωσε το F1 του Δράστη (Actor) κατά +2.7 μονάδες (Πίνακας~\ref{tab:greek-s1-ablations}), επιτρέποντας στο μοντέλο να επιλύσει αναντιστοιχίες γραμματικού/σημασιολογικού ρόλου.
\textit{Παράδειγμα:} ``The public was manipulated by the media.''
\begin{itemize}
    \item \textbf{Standard CoT:} Εξάγει το ``the public'' ως Δράστη (γραμματικό υποκείμενο).
    \item \textbf{DD-CoT:} Διακρίνει ότι το ``media'' είναι ο σημασιολογικός Δράστης και το ``public'' το Θύμα.
\end{itemize}

\paragraph*{Επιτυχία: Μετριασμός Παγίδας Ρεπόρτερ (S2).}
Η Contrastive Retrieval μειώνει τα False Positives ανακτώντας προηγούμενα παραδείγματα που συζητούν συνωμοσίες χωρίς να τις υιοθετούν (Debunking/Reporting), εκπαιδεύοντας το μοντέλο να εστιάζει στην απόδοση πηγών.

\paragraph*{Αποτυχία: Υψηλό Πλαίσιο Ειρωνείας.}
Σενάρια τύπου Poe's Law παραμένουν δύσκολα όταν ο σαρκασμός δεν επισημαίνεται ρητά (π.χ. ``yeah, obviously...''). Η πυκνότητα δεικτών παρερμηνεύεται ως υιοθέτηση απουσία ιστορικού χρήστη.

\subsection*{Μελέτες Αφαίρεσης}

Διεξάγουμε συστηματικές αφαιρέσεις (ablations) για να απομονώσουμε τη συνεισφορά κάθε στοιχείου στο σύνολο ανάπτυξης.

\subsubsection*{Αφαιρέσεις S1 (Εξαγωγή)}
Ο Πίνακας~\ref{tab:greek-s1-ablations} παρουσιάζει την πτώση απόδοσης αφαιρώντας στοιχεία από τον πλήρη αγωγό.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \textbf{Διαμόρφωση}                    & \textbf{S1 Macro F1} & \textbf{$\Delta$} \\
        \midrule
        Πλήρης Αγωγός                          & 0.24                 & ---               \\
        \quad $-$ Βρόχος Αυτο-Βελτίωσης        & 0.18                 & $-$0.06           \\
        \quad $-$ DD-CoT (τυπικό CoT)          & 0.20                 & $-$0.04           \\
        \quad $-$ Αντιπαραβολική Ανάκτηση      & 0.19                 & $-$0.05           \\
        \quad $-$ Στρωματοποιημένη Δειγμ/ψία   & 0.21                 & $-$0.03           \\
        \quad $-$ Ανακατάταξη Cross-Encoder    & 0.22                 & $-$0.02           \\
        \quad $-$ Βελτιστοποίηση GEPA          & 0.20                 & $-$0.04           \\
        \quad $-$ Ντετερμινιστικός Επαληθευτής & 0.16                 & $-$0.08           \\
        \bottomrule
    \end{tabular}
    \caption{Αποτελέσματα αφαίρεσης S1.}
    \label{tab:greek-s1-ablations}
\end{table}

\subsubsection*{Αφαιρέσεις S2 (Ταξινόμηση)}
Ο Πίνακας~\ref{tab:greek-s2-ablations} δείχνει την επίδραση των στοιχείων S2, με έμφαση στον ρυθμό ψευδώς θετικών (FP Rate).

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \textbf{Διαμόρφωση}                  & \textbf{S2 F1} & \textbf{S2 Acc} & \textbf{FP Rate} \\
        \midrule
        Πλήρης Αγωγός                        & 0.79           & 0.84            & 11\%             \\
        \quad $-$ Συμβούλιο (single-pass)    & 0.71           & 0.76            & 24\%             \\
        \quad $-$ Εγκληματολογικός Αναλυτής  & 0.74           & 0.80            & 18\%             \\
        \quad $-$ Δείκτες S1 ως είσοδος      & 0.76           & 0.81            & 14\%             \\
        \quad $-$ Εξόρυξη Δύσκολων Αρνητικών & 0.73           & 0.78            & 21\%             \\
        \quad $-$ Βελτιστοποίηση GEPA        & 0.74           & 0.79            & 16\%             \\
        \quad $-$ Βαθμονόμηση Εμπιστοσύνης   & 0.77           & 0.82            & 13\%             \\
        \bottomrule
    \end{tabular}
    \caption{Αποτελέσματα αφαίρεσης S2.}
    \label{tab:greek-s2-ablations}
\end{table}

\subsubsection*{Δικαστής έναντι Πλειοψηφίας}

Συγκρίνουμε τον Βαθμονομημένο Δικαστή με απλή πλειοψηφία (Πίνακας~\ref{tab:greek-judge}). Ο Δικαστής υπερτερεί σημαντικά στις περιπτώσεις αδιεξόδου (2-2), επιτυγχάνοντας 100\% ακρίβεια μέσω της συντηρητικής απόρριψης.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Μέθοδος}        & \textbf{F1}    & \textbf{Acc}   & \textbf{Deadlock Acc} \\
        \midrule
        Απλή Πλειοψηφία         & 0.638          & 0.779          & 66.7\%                \\
        Βαθμονομημένος Δικαστής & \textbf{0.681} & \textbf{0.805} & \textbf{100.0\%}      \\
        \bottomrule
    \end{tabular}
    \caption{Σύγκριση Δικαστή και Πλειοψηφίας.}
    \label{tab:greek-judge}
\end{table}


\section*{Συμπεράσματα}

\subsection*{Σύνοψη}

Η παρούσα εργασία παρουσίασε ένα πρακτορικό σύστημα βασισμένο σε LLMs για το SemEval-2026 Task 10, που αφορά τόσο την εξαγωγή ψυχογλωσσικών δεικτών συνωμοσίας ως εύρη κειμένου (Υποέργο 1) όσο και την ταξινόμηση υιοθέτησης συνωμοσιακής σκέψης (Υποέργο 2). Η κύρια συνεισφορά αποτελεί η δομημένη σε ροή εργασίας προσέγγιση που αποσυνθέτει αυτές τις προκλήσεις σε εξειδικευμένους πράκτορες με συμπληρωματικές αρμοδιότητες.

Τα αποτελέσματα καταδεικνύουν ότι η δομή ροής εργασίας μπορεί να υποκαταστήσει αλλαγές σε επίπεδο μοντέλου ή δεδομένων σε ψυχογλωσσικά πλαίσια ΕΦΓ. Τα βασικά ευρήματα περιλαμβάνουν: (1) η πρακτορική αγωγός υπερβαίνει σημαντικά τη βασική γραμμή χωρίς τελειοποίηση μοντέλου, (2) τα ντετερμινιστικά στοιχεία (Επαληθευτής, Εγκληματολογικός Αναλυτής) παρέχουν δομικές εγγυήσεις που τα ΜΓΜ μόνα τους δεν μπορούν να προσφέρουν, (3) η αντιδικιστική διαβούλευση υπερτερεί της μονοπέρατης ταξινόμησης, και (4) οι αλληλεπιδράσεις μεταξύ στοιχείων είναι μη-προσθετικές.

\subsection*{Περιορισμοί}

Παρά τις σημαντικές βελτιώσεις, αναγνωρίζονται περιορισμοί:
\begin{itemize}
    \item \textbf{Πραγματολογικά φαινόμενα:} Η ειρωνεία και ο σαρκασμός αποτελούν τη συχνότερη αιτία σφαλμάτων (34\% των σφαλμάτων S2). Η εξάρτηση από ρητούς γλωσσικούς δείκτες στάσης καθιστά το σύστημα ευάλωτο σε πραγματολογική αντιστροφή.
    \item \textbf{Υπόρρητη στάση:} Κείμενα που υιοθετούν θεωρίες συνωμοσίας μέσω επιλεκτικής παρουσίασης στοιχείων, χωρίς ρητές δηλώσεις, αποτελούν πρόκληση.
    \item \textbf{Υπολογιστικό κόστος:} Η πλήρης αγωγός απαιτεί 6 κλήσεις ΜΓΜ ανά κείμενο (Παραγωγός + Κριτικός + Βελτιωτής + 4 Συμβούλοι + Δικαστής), αυξάνοντας σημαντικά κόστος και χρόνο εκτέλεσης.
    \item \textbf{Υποκειμενικότητα ορίων εύρους:} Πολλά σφάλματα ορίων αντανακλούν γνήσια αμφισημία σχολιασμού παρά αποτυχία μοντέλου.
    \item \textbf{Πολιτισμική μεταφερσιμότητα:} Τα πρότυπα προτροπών και τα εγκληματολογικά σήματα βαθμονομούνται σε αγγλόφωνη, δυτικοκεντρική συνωμοσιακή ρητορική.
    \item \textbf{Περιορισμοί Υλικού:} Η απουσία υποδομής GPU απέκλεισε τη δυνατότητα τελειοποίησης (fine-tuning) ανοιχτών μοντέλων, καθιστώντας μονόδρομο τη χρήση εμπορικών API και εστιάζοντας την καινοτομία στη μηχανική προτροπών και την πρακτορική αρχιτεκτονική.
    \item \textbf{Εξάρτηση από API:} Η βασισμένη σε API αρχιτεκτονική εισάγει κινδύνους διαθεσιμότητας, κόστους και αναπαραγωγιμότητας λόγω αδιαφανών αναβαθμίσεων των εμπορικών μοντέλων.
    \item \textbf{Μεροληψία Σχολιασμού:} Η εγγενής υποκειμενικότητα στον ορισμό των ορίων (span boundaries) εισάγει θόρυβο στην αξιολόγηση, ιδιαίτερα στις κατηγορίες \textsc{Αποτέλεσμα} και \textsc{Στοιχεία} όπου η συμφωνία σχολιαστών είναι χαμηλότερη.
\end{itemize}

\subsection*{Μελλοντικές Κατευθύνσεις}

Αρκετές κατευθύνσεις θα μπορούσαν να επεκτείνουν αυτή την εργασία:
\begin{enumerate}
    \item \textbf{Ανάλυση ευαίσθητη σε σαρκασμό:} Προσθήκη ρητού σταδίου ανίχνευσης σαρκασμού πριν τη διαβούλευση του συμβουλίου, αξιοποιώντας πρόσφατες εξελίξεις στην ανίχνευση ειρωνείας μέσω αντιπαραβολικής μάθησης.
    \item \textbf{Προσαρμοστική σύνθεση συμβουλίου:} Δυναμική επιλογή προσώπων ενόρκων βάσει χαρακτηριστικών εγγράφου (subreddit, θέμα, γλωσσική πολυπλοκότητα) αντί σταθερού τετραμερούς συμβουλίου.
    \item \textbf{Διαγλωσσική επέκταση:} Προσαρμογή σε πολυγλωσσική ανίχνευση συνωμοσιών, απαιτώντας πολιτισμικά προσαρμοσμένα εγκληματολογικά σήματα αλλά διατηρώντας τη βασική πρακτορική αρχιτεκτονική.
    \item \textbf{Λεπτομερής ταξινόμηση στάσης:} Επέκταση πέραν της δυαδικής ταξινόμησης σε φάσμα (υιοθέτηση, μερική υιοθέτηση, ουδέτερη συζήτηση, αποδόμηση) για καλύτερη αποτύπωση της πραγματολογικής πολυπλοκότητας.
    \item \textbf{Βελτιστοποίηση αποδοτικότητας:} Τεχνικές όπως αποθήκευση απαντήσεων (caching), πρώιμη έξοδος για περιπτώσεις υψηλής εμπιστοσύνης, και χρήση μικρότερων τοπικών μοντέλων βελτιστοποιημένων με LoRA/QLoRA για αρχικό φιλτράρισμα.
\end{enumerate}

\subsection*{Ευρύτερη Επίδραση}

Εξ όσων γνωρίζουμε, η εργασία αυτή αποτελεί την \textit{πρώτη πρακτορική μέθοδο βασισμένη σε ΜΓΜ} για εξαγωγή και ανίχνευση ψυχογλωσσικών δεικτών συνωμοσίας. Τα αποτελέσματα καταδεικνύουν ότι η προσεκτική δόμηση ροών εργασίας μπορεί να επιτύχει σημαντικές βελτιώσεις χωρίς αλλαγές σε επίπεδο μοντέλου, προτείνοντας ένα πρότυπο σχεδίασης όπου η μηχανική προσπάθεια μετατοπίζεται από την εκπαίδευση μοντέλων στην αρχιτεκτονική ροών εργασίας. Αυτό έχει ευρύτερες επιπτώσεις πέραν της ανίχνευσης συνωμοσιών: κάθε εργασία ΕΦΓ που απαιτεί τόσο σημασιολογικό συλλογισμό όσο και δομική ακρίβεια θα μπορούσε να ωφεληθεί από την υβριδική ντετερμινιστική-ΜΓΜ προσέγγιση που αναπτύχθηκε στην παρούσα εργασία.
