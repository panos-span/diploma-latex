\section{Development Trajectory}
\label{sec:development-trajectory}

This section documents the iterative development process spanning five months (September 2025--January 2026) through 42 git commits, revealing how the architecture evolved from naive baselines to the current agentic system.


\subsection{Evolution Timeline}

The development followed six distinct phases, each representing a paradigm shift in approach:

\paragraph{Phase 1: Data Understanding (September--October 2025).}
Initial work focused on exploratory data analysis (EDA) to understand the task characteristics. Key insights from this phase:
\begin{itemize}[noitemsep]
    \item Discovered severe class imbalance: \textit{Evidence} and \textit{Victim} markers are rare ($<$15\% of spans)
    \item Identified the \textit{Reporter Trap}: Reddit Submission Statements often describe conspiracies without endorsing them
    \item Built the data pipeline for rehydrating redacted social media posts
    \item Developed comprehensive EDA visualizations documenting marker distributions, text length correlations, and annotation overlaps (Section~\ref{sec:eda})
\end{itemize}

\paragraph{Phase 2: Baseline Establishment (October 2025).}
AWS Bedrock integration with Claude Sonnet 4.5~\cite{anthropic2025claude} established initial LLM baselines:
\begin{itemize}[noitemsep]
    \item Zero-shot prompting achieved S1 F1 = 0.12, S2 Accuracy = 0.71
    \item Prompt sweeping tested systematic variations across temperature, persona framing, and output formats
    \item XML tags improved Claude's adherence to structured output requirements
    \item Claude Haiku 4 was used for rapid prototyping due to its lower cost and faster inference
\end{itemize}

\paragraph{Phase 3: Prompt Engineering (October--November 2025).}
Intensive prompt iteration with focus on span extraction precision:
\begin{itemize}[noitemsep]
    \item Developed deterministic span verification to eliminate hallucinated extractions
    \item Introduced the ``hybrid approach'': LLM extraction + programmatic validation
    \item Achieved first significant S1 improvement (F1: 0.12 $\rightarrow$ 0.20) through granularity constraints
    \item Added few-shot examples with explicit rationales explaining \textit{why} each span was labeled
    \item Experimented with XML-structured vs. plain-text prompts, finding XML superior for Claude
\end{itemize}

\paragraph{Phase 4: Agentic Architecture (November--December 2025).}
Transition from monolithic prompts to multi-agent workflows:
\begin{itemize}[noitemsep]
    \item Implemented Pydantic-AI~\cite{pydanticai2024} for structured outputs, eliminating JSON parsing failures
    \item Added self-consistency ensemble ($k=3$) with majority voting
    \item Developed LangGraph~\cite{langgraph2024} ``Legislator-Judge'' system for S2 with adversarial debate
    \item Introduced ReX-GoT (Reasoning \& Execution Graph-of-Thought) for complex reasoning chains
    \item Built RAG system with batch inference for few-shot retrieval using ChromaDB~\cite{chromadb2023}
\end{itemize}

\paragraph{Phase 5: Automated Optimization (December 2025--January 2026).}
GEPA integration for systematic prompt improvement:
\begin{itemize}[noitemsep]
    \item Integrated MLflow for experiment tracking and prompt versioning
    \item Developed custom scorers with ``Trojan Horse'' pattern for rich feedback (Section~\ref{sec:gepa-details})
    \item Dynamic RAG context injection based on query similarity
    \item Phased optimization: Generator $\rightarrow$ Critic $\rightarrow$ Refiner (S1), Council $\rightarrow$ Judge (S2)
\end{itemize}

\paragraph{Phase 6: Architecture Consolidation (January 2026).}
Migration to OpenAI GPT-5.2 and final system refinement:
\begin{itemize}[noitemsep]
    \item Ported prompts from XML (Claude-optimized) to Markdown (GPT-optimized)
    \item Simplified S1 to single DD-CoT agent + deterministic tools (reduced token cost by 60\%)
    \item Added appeal/retrial mechanism in S2 Judge for borderline cases
    \item Finalized Anti-Echo Chamber parallel council architecture
\end{itemize}


\subsection{Performance Progression}

Table~\ref{tab:dev-performance} summarizes key milestones on the held-out development set, mapped to the git commit timeline.

\begin{table}[H]
\centering
\caption{Key performance milestones mapped to development timeline. Best results in bold.}
\label{tab:dev-performance}
\begin{tabular}{@{}lllcc@{}}
\toprule
\textbf{Date} & \textbf{Commit} & \textbf{Innovation} & \textbf{S1 F1} & \textbf{S2 Acc} \\
\midrule
Oct 12 & Bedrock Integration & Zero-Shot Baseline & 0.12 & 0.71 \\
Oct 23 & Prompt Sweep & Few-Shot + CoT & 0.14 & 0.67 \\
Oct 28 & Deterministic Verifier & Hybrid Extraction & 0.18 & 0.74 \\
Oct 30 & Granularity Fix & \textbf{S1 Highscore v1} & 0.20 & 0.75 \\
Nov 12 & Pydantic-AI & Structured Outputs & 0.20 & 0.79 \\
Nov 28 & Self-Consistency & Ensemble ($k=3$) & 0.22 & 0.78 \\
Nov 29 & LangGraph S2 & Legislator-Judge & 0.22 & 0.78 \\
Dec 03 & RAG System & Contrastive Few-Shot & \textbf{0.24} & 0.79 \\
Dec 29 & GEPA Alpha & Automated Optimization & 0.23 & \textbf{0.79} \\
Jan 26 & DD-CoT System & Final Architecture & \textbf{0.24} & 0.80 \\
Jan 31 & Simplified S1 & Single Agent + Tools & 0.24 & \textbf{0.81} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    phase/.style={draw, rounded corners, fill=blue!8, minimum width=1.6cm, minimum height=1.2cm, align=center, font=\footnotesize},
    final/.style={draw, rounded corners, fill=green!15, minimum width=2cm, minimum height=1.2cm, align=center, font=\footnotesize},
    arr/.style={-{Stealth[length=3mm]}, thick, gray},
    date/.style={font=\scriptsize\itshape, text=gray}
]
    \node[phase] (p1) at (0,0) {EDA\\Analysis};
    \node[phase] (p2) at (2.5,0) {Bedrock\\LLM};
    \node[phase] (p3) at (5,0) {Prompt\\Eng.};
    \node[phase] (p4) at (7.5,0) {Pydantic\\AI};
    \node[phase] (p5) at (10,0) {LangGraph\\Agents};
    \node[phase] (p6) at (12.5,0) {GEPA\\+ OpenAI};
    
    \node[final] (pf) at (12.5,-2) {DD-CoT +\\Anti-Echo\\Chamber};
    
    \draw[arr] (p1) -- (p2);
    \draw[arr] (p2) -- (p3);
    \draw[arr] (p3) -- (p4);
    \draw[arr] (p4) -- (p5);
    \draw[arr] (p5) -- (p6);
    \draw[arr] (p6) -- (pf);
    
    \node[date] at (0,-0.95) {Oct 5};
    \node[date] at (2.5,-0.95) {Oct 12};
    \node[date] at (5,-0.95) {Oct 28};
    \node[date] at (7.5,-0.95) {Nov 12};
    \node[date] at (10,-0.95) {Nov 29};
    \node[date] at (12.5,-0.95) {Jan 14};

    \draw[thick, gray!50, dashed] (-1.2,1) -- (13.7,1);
    \node[font=\scriptsize, text=gray] at (-0.5,1.25) {Sep 2025};
    \node[font=\scriptsize, text=gray] at (13,1.25) {Feb 2026};
\end{tikzpicture}
\caption{System evolution timeline derived from 42 git commits across 5 months.}
\label{fig:dev-timeline}
\end{figure}


\subsection{Error Analysis}
\label{sec:error-analysis}

Systematic error analysis on the development set revealed four critical failure modes that informed our architectural decisions.

\paragraph{S1: Label Confusion.} Exploratory data analysis revealed that \textbf{Action $\leftrightarrow$ Effect} is the most confusing pair, followed by \textbf{Actor $\leftrightarrow$ Victim} in passive constructions. For example, in ``The people are being poisoned by the government,'' the LLM frequently classified ``the government'' as both Actor (correct) and as a contextual entity without a label. Similarly, ``being poisoned'' straddles the boundary between Action (the mechanism) and Effect (the outcome). The DD-CoT discriminative reasoning directly addresses this by forcing explicit ``why NOT other label'' justification.

\paragraph{S1: Phantom Spans.} Early LLM approaches hallucinated spans not present in the source text. For instance, the model might extract ``government conspiracy'' when the source text only contained ``the government did X'' without the word ``conspiracy.'' The deterministic Verifier cascade (exact match $\rightarrow$ case-insensitive $\rightarrow$ normalized $\rightarrow$ fuzzy $\rightarrow$ LCS), introduced in the October 28 commit, reduced phantom spans to near-zero while preserving recall.

\paragraph{S2: The Reporter Trap.} Single-agent classifiers confused \textit{topic presence} with \textit{endorsement}. News articles and Reddit Submission Statements discussing conspiracy theories were systematically misclassified as ``conspiracy'' even when the author was merely reporting on conspiratorial claims. The Anti-Echo Chamber architecture, with its dedicated Defense Attorney and Reporter Defense prompting, specifically addresses this failure mode.

\paragraph{S2: Sequential Debate Bias.} The initial LangGraph ``Legislator-Judge'' system (November 29) used sequential debate where later agents saw earlier arguments, introducing ordering bias---later agents disproportionately agreed with earlier agents. The parallel council architecture (January 2026) ensures independent voting by executing all four personas concurrently via \texttt{asyncio.gather} without information leakage.


\subsection{Key Architectural Insights}

The iterative development process yielded five key insights that shaped the final architecture:

\begin{enumerate}
    \item \textbf{Single Well-Prompted Agent $>$ Ensemble for S1:} Self-consistency with $k=3$ provided diminishing returns compared to the DD-CoT + Critic + Refiner pipeline, which achieves comparable quality with fewer tokens by decomposing the problem into specialized reasoning stages.
    
    \item \textbf{Multi-Agent Essential for S2:} Unlike S1, S2 benefits from genuine perspective diversity. The parallel council prevents the ``echo chamber'' effect where a single agent locks into an initial interpretation, particularly for borderline cases involving reporting vs. endorsement.
    
    \item \textbf{Structured Outputs Critical:} The transition to Pydantic-AI (Phase 4) yielded immediate gains (S1: 0.14 $\rightarrow$ 0.20, S2: 0.77 $\rightarrow$ 0.79) by eliminating JSON parsing failures and enforcing schema constraints.
    
    \item \textbf{RAG Requires Contrastive Examples:} Standard few-shot retrieval based on similarity alone was insufficient. Hard negative mining and contrastive rationale generation (Phase 5) taught discrimination rather than pattern matching.
    
    \item \textbf{Confidence Calibration Matters:} Programmatic confidence damping based on council consensus enabled reliable uncertainty quantification for downstream decision-making, preventing overconfident misclassifications.
\end{enumerate}
